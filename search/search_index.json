{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Projeto pessoal para consulta desde Guia de Modelagem,  boas pr\u00e1ticas, at\u00e9 Study Notes, Links e Artigos interessantes e suas respectivas refer\u00eancias</p>"},{"location":"A_modeling_guideline/A_overview/","title":"\ud83d\udcdd Overview","text":"<p>Before diving into the modeling process, it\u2019s important to recognize that every model is a handcrafted product. This guide is not a recipe, but a foundation\u2014a structured set of good practices to support your work and help you build models that are robust, interpretable, and valuable.</p> <p>Throughout each step, validate your work incrementally. If you can\u2019t validate an entire pipeline immediately, validate small samples to ensure you\u2019re heading in the right direction.</p> <p>This material was developed primarily with supervised models in a banking context in mind, but it also includes insights that are applicable to unsupervised learning and general modeling strategy.</p> <pre><code>\ud83d\udccc Regardless of the algorithm chosen, the greatest value in a modeling project \n    comes from decisions made in the early stages:\n</code></pre> <ul> <li>Defining a target variable that accurately reflects the business problem</li> <li>Selecting an eligible population that truly represents the operational context</li> <li>Creating relevant, informative features that capture the underlying patterns</li> <li>Selecting the most appropriate subset of features for the model</li> <li>Defining a sampling window aligned with business cycles and data reality</li> </ul> <p>While cutting-edge algorithms can provide marginal performance gains, they are rarely the defining factor in a successful modeling project. A well-defined problem, solid data, and thoughtful feature engineering will always be the real drivers of impact.</p>"},{"location":"A_modeling_guideline/B_problem_understanding/","title":"\u2753 Problem Understanding","text":"<p>Before starting any modeling effort, it\u2019s crucial to deeply understand the business problem and the true need behind it. Many data science projects fail not because of poor modeling, but because the problem was poorly framed from the outset.</p> <p>Not every challenge requires a complex machine learning solution\u2014often, a simple rule-based approach or heuristic can deliver value faster and with less overhead. In some cases, a lightweight baseline model may be the best starting point. On the other hand, problems involving long-term predictions, personalization, or dynamic systems may benefit from more sophisticated models with greater complexity and precision.</p> <p>To guide this understanding, we break the process into three key steps:</p>"},{"location":"A_modeling_guideline/B_problem_understanding/#discovery-insight","title":"\ud83d\udd0d Discovery &amp; Insight","text":"<p>This phase focuses on aligning with stakeholders to clearly define the business objective and ensure the team is solving the right problem. It\u2019s not just about understanding what the stakeholder wants, but uncovering why they want it (e.g., reduce churn, predict default, optimize acquisition).</p> <p>Start by partnering with the business area to define:</p> <ul> <li> <p>The objective: What is our goal? What decision are we trying to support, automate, or improve? What\u2019s the limitation of the current process or model? What pain point are we addressing?</p> <pre><code>\u27a4 Tip: Analyze historical data and business processes to identify \n   bottlenecks and root causes.\n</code></pre> </li> <li> <p>The expected impact: How will this solution create value? Examples include reducing financial risk, increasing revenue, or improving operational efficiency. What actions or decisions will be driven by the model\u2019s output?</p> </li> <li> <p>Constraints and requirements: Are there deadlines, regulatory rules (e.g., fairness, explainability), technical restrictions, or data access issues?</p> </li> </ul> <p>Because this step sets the foundation for the entire project, it's critical to ensure nothing important is overlooked. That includes:</p> <ul> <li> <p>Clarifying success metrics: both business KPIs (e.g., approval rate, NPS, default rate) and technical performance metrics (e.g., AUC, KS, precision, recall)</p> </li> <li> <p>Understanding the decision context: who will use the model, how often, and how the output will be interpreted and acted upon</p> </li> <li> <p>Scoping the solution space: determine whether a business rule, statistical model, or machine learning solution is most appropriate given the complexity, timeline, and available data</p> <pre><code>\ud83d\udccc Best Practice: If available, review existing dashboards, reports,\n    previous models, or experiment results to understand what\u2019s already\n    been tried and what gaps remain.\n</code></pre> </li> </ul> <p>On this step we present a Discovery</p>"},{"location":"A_modeling_guideline/B_problem_understanding/#target-definition","title":"\ud83c\udfaf Target Definition","text":"<p>Once the problem is well understood, the next step is to define the target variable\u2014the outcome the model will predict and the basis for measuring success. A well-defined target ensures that the model output is not only useful, but also measurable, actionable, and aligned with business value.</p> <p>\u2705 Best Practices: - Choose a target that reflects real business value: The outcome should be directly tied to the decision-making process and clearly represent the goal (e.g., customer default, conversion, retention).</p> <ul> <li> <p>Ensure it is observable and measurable: You must be able to label historical data reliably and consistently. If the outcome isn\u2019t recorded or is ambiguous, model performance will suffer.</p> </li> <li> <p>Define the observation window: Determine how far into the future you're trying to predict. This affects both the sampling strategy and the feature engineering process.</p> </li> <li> <p>Select the appropriate target type: Based on the problem, decide if your task is:</p> <ul> <li>Binary classification (e.g., default vs. non-default)<ul> <li>Ever: refers to any occurrence during that period/ Existence at any point within the window.  </li> <li>Over: refers to the cumulative or final state by the end of that period/ Aggregate/result by the end of the window.</li> </ul> </li> <li>Regression (e.g., expected revenue, days to churn)</li> <li>Multi-class classification (e.g., customer segments, fraud types)</li> </ul> </li> <li> <p>Align on evaluation metrics: Choose metrics that reflect both model quality and business impact\u2014e.g., precision, recall, AUC-ROC, uplift, or financial return.</p> <pre><code>\u26a0\ufe0f Avoid proxy targets that don\u2019t reflect the real objective. For example, \n    using clicks to represent user satisfaction can mislead the modeling \n    process if the two aren\u2019t strongly correlated.\n</code></pre> </li> </ul>"},{"location":"A_modeling_guideline/B_problem_understanding/#solution-definition","title":"\ud83e\udde9 Solution Definition","text":"<p>This step encourages pragmatism\u2014simpler approaches like business rules or heuristics can often outperform complex models, especially in early iterations. These solutions are typically faster to implement, easier to explain, and sufficient for generating business value.</p> <p>Start by identifying how the model or solution will be used and what impact it should create. Then evaluate trade-offs between model complexity, time-to-value, and operational constraints.</p> <pre><code>\ud83d\udca1 Ask: What is the minimum viable solution that delivers reliable value \n    with acceptable effort and risk?\n</code></pre> <p>When appropriate, deploy a quick-win baseline model\u2014such as logistic regression or decision trees\u2014and evolve from there as needed.</p> <ul> <li> <p>Start simple: Consider whether a rule set or scorecard meets the need\u2014particularly if interpretability, regulatory approval, or fast deployment is a priority.</p> </li> <li> <p>Deploy baseline models early: Techniques like logistic regression or shallow decision trees provide quick, interpretable wins and can serve as solid baselines.</p> </li> <li> <p>Assess feasibility and maintainability: Consider the effort to maintain and retrain complex models over time. Sometimes, operational simplicity outweighs marginal gains in accuracy.</p> <pre><code>\ud83d\udee0\ufe0f Example: In credit risk modeling, a scorecard with well-defined thresholds \n    may be more practical and trusted by risk committees than a black-box \n    ensemble model\u2014especially when regulatory transparency is required.\n</code></pre> </li> </ul>"},{"location":"A_modeling_guideline/B_problem_understanding/#optional-enhancements-only-if-you-want-to-expand-further","title":"\u2728 Optional Enhancements (Only If You Want to Expand Further)","text":""},{"location":"A_modeling_guideline/B_problem_understanding/#stakeholder-map","title":"Stakeholder Map","text":"<p>List who is involved and who will use the model output (analysts, business teams, product, legal). This helps align communication and expectations.</p>"},{"location":"A_modeling_guideline/B_problem_understanding/#decision-journey-mapping","title":"Decision Journey Mapping","text":"<p>Show where in the process the model will impact decisions (e.g., credit approval, campaign selection). This clarifies integration points.</p>"},{"location":"A_modeling_guideline/B_problem_understanding/#riskfeasibility-assessment","title":"Risk/Feasibility Assessment","text":"<p>Include a short check for data availability, regulatory limits (e.g., fair lending laws), or black-box explainability concerns.</p>"},{"location":"A_modeling_guideline/B_problem_understanding/#baselinebenchmark-expectations","title":"Baseline/Benchmark Expectations","text":"<p>What existing rule or model exists? What lift is expected? This sets realistic expectations.</p>"},{"location":"A_modeling_guideline/C_data_preparation/","title":"\ud83d\udcca Data Understanding and Preparation","text":""},{"location":"A_modeling_guideline/C_data_preparation/#data-collection","title":"Data Collection","text":""},{"location":"A_modeling_guideline/C_data_preparation/#data-cleaning-and-preprocessing","title":"Data Cleaning and Preprocessing","text":""},{"location":"A_modeling_guideline/D_eda/","title":"\ud83d\udd0d Exploratory Data Analysis","text":"<p>a</p>"},{"location":"A_modeling_guideline/D_eda/#teste","title":"teste","text":""},{"location":"A_modeling_guideline/E_feature_engineering/","title":"\ud83d\udee0\ufe0f Feature Engineering","text":""},{"location":"A_modeling_guideline/E_feature_engineering/#feature-engineering","title":"Feature Engineering","text":""},{"location":"A_modeling_guideline/F_model_selection/","title":"\ud83e\uddee Modeling and Evaluation or Model Design","text":"<p>This is the design phase</p>"},{"location":"A_modeling_guideline/F_model_selection/#development-and-hyperparameter-tuning","title":"\ud83e\udde0 Development and Hyperparameter Tuning","text":"<p>The algorithm(s) used, architectures if relevant (e.g., XGBoost, Neural Net).</p> <p>Train/test splits, metrics used, hyperparameters tuning, validation strategy (CV, OOT, etc).</p>"},{"location":"A_modeling_guideline/F_model_selection/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>A busca de hiperpar\u00eametro \u00e9 importante para...Temos diversas t\u00e9cnicas de busca para otimiza\u00e7\u00e3o de hiperpar\u00e2metros</p>"},{"location":"A_modeling_guideline/F_model_selection/#grid-search","title":"Grid Search","text":"<p>O Grid Search testa todas as combina\u00e7\u00f5es poss\u00edveis de hiperpar\u00e2metros dentro de um grid definido por voc\u00ea.</p> <pre><code>param_grid = {\n    'learning_rate': [0.01, 0.1],\n    'max_depth': [3, 5, 7]\n}\n</code></pre> <p>\u2192 Total de combina\u00e7\u00f5es: 2 \u00d7 3 = 6 modelos treinados</p> <p>\u2705 Vantagens:</p> <ul> <li>Explora exaustivamente o espa\u00e7o definido.</li> <li>\u00datil quando voc\u00ea tem poucos par\u00e2metros e valores espec\u00edficos em mente.</li> </ul> <p>\u274c Desvantagens:</p> <ul> <li>Muito lento se o n\u00famero de combina\u00e7\u00f5es for alto.</li> <li>Pode gastar tempo testando combina\u00e7\u00f5es irrelevantes ou redundantes.</li> </ul> <p>Exemplo de c\u00f3digo utilizando valida\u00e7\u00e3o cruzada e RMSE como m\u00e9trica principal</p> <pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import make_scorer, mean_squared_error\n\n# Fun\u00e7\u00e3o de erro RMSE (quanto menor, melhor)\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)\n\nrmse_scorer = make_scorer(rmse, greater_is_better=False)\n\n# Grade de hiperpar\u00e2metros\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_iter': [300, 500], #Evite max_iter muito alto no in\u00edcio \u2014 use valores moderados para teste r\u00e1pido.\n    'max_leaf_nodes': [15, 31],\n    'min_samples_leaf': [10, 20],\n    'l2_regularization': [0.0, 1.0]\n}\n\n# Grid Search com valida\u00e7\u00e3o cruzada\ngrid_search = GridSearchCV(\n    estimator=HistGradientBoostingRegressor(),\n    param_grid=param_grid,\n    scoring=rmse_scorer,\n    cv=3,\n    verbose=2,\n    n_jobs=-1 #usar todos os n\u00facleos do processador.\n)\n\n# Executa busca\ngrid_search.fit(X, y)\n\n# Mostra melhor combina\u00e7\u00e3o\nprint(\"Melhores hiperpar\u00e2metros encontrados:\")\nprint(grid_search.best_params_)\n\nprint(f\"Melhor RMSE m\u00e9dio (valida\u00e7\u00e3o cruzada): {-grid_search.best_score_:.4f}\")\n\n# Modelo otimizado\nbest_model = grid_search.best_estimator_\n</code></pre>"},{"location":"A_modeling_guideline/F_model_selection/#randomized-search","title":"Randomized Search","text":"<p>O Randomized Search seleciona aleatoriamente combina\u00e7\u00f5es de hiperpar\u00e2metros dentro de um grid definido, por um n\u00famero fixo de tentativas (n_iter).</p> <pre><code>param_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 5, 7],\n    'min_samples_leaf': [10, 20, 30]\n}\n</code></pre> <p>\u2192 Poss\u00edveis combina\u00e7\u00f5es = 3 \u00d7 3 \u00d7 3 = 27 \u2192 Mas voc\u00ea define n_iter=10, ent\u00e3o s\u00f3 10 combina\u00e7\u00f5es aleat\u00f3rias ser\u00e3o testadas</p> <p>\u2705 Vantagens:</p> <ul> <li>Muito mais eficiente em tempo.</li> <li>Pode encontrar bons resultados rapidamente mesmo em grids grandes.</li> <li>Permite testar mais par\u00e2metros simultaneamente com menos custo.</li> </ul> <p>\u274c Desvantagens:</p> <ul> <li>Pode n\u00e3o testar a melhor combina\u00e7\u00e3o poss\u00edvel, pois \u00e9 aleat\u00f3rio.</li> <li>Resultados variam a cada execu\u00e7\u00e3o (a menos que defina random_state).</li> </ul>"},{"location":"A_modeling_guideline/F_model_selection/#bayesina-optimization","title":"Bayesina Optimization","text":"<p>Podemos utilizar o optuna ou hyperopt</p>"},{"location":"A_modeling_guideline/F_model_selection/#convex-algorithms","title":"Convex Algorithms","text":""},{"location":"A_modeling_guideline/F_model_selection/#genetic-algorithms","title":"Genetic Algorithms","text":""},{"location":"A_modeling_guideline/F_model_selection/#hyperband","title":"Hyperband","text":""},{"location":"A_modeling_guideline/F_model_selection/#overview","title":"Overview","text":"Situa\u00e7\u00e3o Melhor escolha Poucos hiperpar\u00e2metros e valores espec\u00edficos Grid Search Muitos hiperpar\u00e2metros ou ranges grandes Randomized Search Tempo de execu\u00e7\u00e3o limitado Randomized Search Busca precisa e exaustiva \u00e9 prioridade Grid Search"},{"location":"A_modeling_guideline/F_model_selection/#validation-and-evaluation","title":"\u2705 Validation and Evaluation","text":"<p>Detailed evaluation: metrics results (ROC AUC, F1-score, KS, etc.), important variables.</p> <p>Desvio padrao da feature para olhar drift feature a feature juntamente de KS, AUC e PR-AUC</p> <p>https://christophm.github.io/interpretable-ml-book/</p>"},{"location":"A_modeling_guideline/F_model_selection/#model-bias-and-fairness-analysis","title":"Model Bias and Fairness Analysis","text":""},{"location":"A_modeling_guideline/F_model_selection/#interpretation-and-explanability","title":"\ud83d\udca1 Interpretation and Explanability","text":""},{"location":"A_modeling_guideline/J_deployment/","title":"\ud83d\ude80 Deployment and Operationalization","text":"<p>a</p>"},{"location":"A_modeling_guideline/K_monitoring/","title":"\ud83d\udcc8 Model Monitoring and Maintenance","text":"<p>How model performance and drift will be monitored after go-live.</p> <p>What are the known issues, assumptions, and limitations? Can you monitor something to this issues</p>"},{"location":"A_modeling_guideline/K_monitoring/#1-prediction-quality-monitoring","title":"\ud83d\udcc8 1. Prediction Quality Monitoring","text":"<p>Monitor if the model predictions start behaving differently:</p> What to Monitor How to Monitor Why Score Distribution Plot the distribution of prediction scores (histogram). Detect if model outputs are drifting. Population Stability Index (PSI) Compare score distributions over time. Identify if input population is changing. Performance Metrics Calculate metrics (e.g., AUC, KS, Precision, Recall) periodically. See if the model is getting worse at predicting."},{"location":"A_modeling_guideline/K_monitoring/#2-input-data-monitoring-data-drift","title":"\ud83d\udee0 2. Input Data Monitoring (Data Drift)","text":"<p>Monitor the input variables going into your model:</p> What to Monitor How to Monitor Why Feature Distribution Drift Track mean, std, min, max of key features. Compare with training. Detect if the real-world data is changing. Missing Values Rate Monitor missing data per feature. Rising missingness may crash or bias your model. Feature Correlations Monitor if correlations between variables shift heavily. Indicates data relationships are changing."},{"location":"A_modeling_guideline/K_monitoring/#3-model-operational-monitoring","title":"\u26a0\ufe0f 3. Model Operational Monitoring","text":"<p>Monitor the operational and technical aspects:</p> What to Monitor How to Monitor Why Latency Measure prediction time. To catch slowdowns or bottlenecks. Throughput Number of predictions made per second/minute. To detect overloads. Error Rates Log any errors during predictions. To detect technical failures (bad inputs, infra issues)."},{"location":"A_modeling_guideline/K_monitoring/#4-alerts-setup","title":"\ud83d\udea8 4. Alerts Setup","text":"<p>You should automatically trigger alerts when something goes wrong:</p> <ul> <li>If PSI &gt; 0.25 \u2192 possible drift \u2192 send alert</li> <li>If AUC drops below threshold \u2192 send alert</li> <li>If missing data &gt; 10% in key features \u2192 send alert</li> <li>If model response time &gt; X ms \u2192 send alert</li> </ul> <p>You can configure alerts using tools like:</p> <ul> <li>Datadog</li> <li>Prometheus + Grafana</li> <li>AWS CloudWatch</li> <li>Or even simple scripts with emails/slack notifications</li> </ul>"},{"location":"A_modeling_guideline/K_monitoring/#5-best-practices","title":"\ud83e\udde0 5. Best Practices","text":"<p>\u2705 Monitoring is separate from training</p> <p>\u2705 Monitoring runs automatically</p> <p>\u2705 Monitoring dashboards are shared with Data Science and MLOps</p> <p>\u2705 Monitoring results are audited (especially if you're in banking/regulated sectors)</p>"},{"location":"A_modeling_guideline/K_monitoring/#bonus-tools-that-help-you-monitor-automatically","title":"\ud83d\ude80 Bonus: Tools that help you monitor automatically","text":"<ul> <li>Evidently AI (Open-source, great for drift monitoring)</li> <li>WhyLabs (SaaS monitoring for ML models)</li> <li>Neptune.ai (Experiment and model monitoring)</li> </ul>"},{"location":"A_modeling_guideline/L_governance/","title":"\u2696\ufe0f Governance, Ethical, and Regulatory Considerations","text":""},{"location":"A_modeling_guideline/L_governance/#model-governance","title":"Model Governance","text":"Aspect What to include Why it's important Ownership Name the owner (Data Scientist, Product Manager) responsible for the model. Accountability. Version Control Keep a changelog of model versions. Track improvements and regressions. Approval Process Who needs to sign off before going live (e.g., Risk, Compliance)? Governance and traceability. Model Risk Tier Classify risk level (e.g., Low, Medium, High) based on business impact. Prioritize monitoring and controls. <pre><code>Example from Revolut:\n\"Model Owner: Igor Braga (Credit Data Scientist). Model Risk Tier: Medium. \n Approved by Credit Risk Committee (April 2025).\"\n</code></pre>"},{"location":"A_modeling_guideline/L_governance/#ethical-considerations","title":"Ethical Considerations","text":"<p>LGPD</p> Aspect What to include Why it's important Fairness and Bias Testing Test for bias across gender, age, ethnicity, etc. (if possible). Avoid discrimination and unfair practices. Explainability How understandable are the model's decisions? Are tools like SHAP used? Improve trust and user understanding. Impact Analysis Analyze potential harm or negative side-effects. Mitigate unintended consequences. <pre><code>Example sentence Revolut would use:\n\"No statistically significant bias found across gender or age groups. \n SHAP values are used for local interpretability at individual decision level.\"\n</code></pre>"},{"location":"A_modeling_guideline/L_governance/#regulatory-compliance","title":"Regulatory Compliance","text":"<p>Sox</p> <p>IFRS9</p> Aspect What to include Why it's important Data Privacy and Protection Does the model handle personal data? Is it GDPR-compliant? Legal requirement. Right to Explanation Under GDPR, customers may ask for explanation of decisions. Can your model provide it? Regulatory requirement (GDPR Article 22). Record Keeping Keep full audit logs of model outputs and decisions. For regulatory inspections and audits. <p>Example sentence Revolut would use: \"The model complies with GDPR, ensuring no processing of sensitive personal attributes. Audit logs are maintained for 12 months.\"</p>"},{"location":"A_modeling_guideline/M_tools/","title":"\ud83e\uddf0 Tools, Technologies, and Best Practices","text":"<p>a</p>"},{"location":"A_modeling_guideline/modeling_guideline/","title":"Introduction","text":"<p>Antes de iniciar esse direcionamento para Modelagem, lembre-se que todo modelo \u00e9 fruto de um trabalho artesanal, e esse material \u00e9 meramente um pilar para sua obra...</p> <p>Ap\u00f3s cada passo valide seu trabalho, caso n\u00e3o consiga validar todos, ao menos 10 casos para ter alguma ideia se o que est\u00e1 fazendo est\u00e1 caminhando da forma esperada.</p> <p>O conte\u00fado desse material foi desenvolvido pensando principalmente em modelos supervisionados, tendo espa\u00e7o para alguns insights sobre modelos n\u00e3o supervisionados.</p> <p>\u00c9 importante lembrarmos que independente do algoritmo que utilizarmos posteriormente, o maior ganho que teremos ser\u00e1 durante os primeiros passos da modelagem:</p> <ul> <li>na defini\u00e7\u00e3o de um target aderente ao problema que queremos solucionar</li> <li>na defini\u00e7\u00e3o de p\u00fablico eleg\u00edvel fiel ao nosso contexto real</li> <li>na cria\u00e7\u00e3o de features que, em conjunto, possam explicar melhor nosso modelo</li> <li>na sele\u00e7\u00e3o das melhores features para nosso contexto</li> <li>em m\u00e9todos eficazes e entendimento do neg\u00f3cio para definir o per\u00edodo a ser considerado na amostragem</li> </ul> <p>A utiliza\u00e7\u00e3o de algoritmos de ponta n\u00e3o ser\u00e1 nosso 'game change', ele pode nos garantir aquele ponto de m\u00e9trica a mais em nosso modelo, mas n\u00e3o ser\u00e1 o protagonista da modelagem.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#0-entendimento-do-problema","title":"0 - Entendimento do Problema","text":"<p>Antes de mais nada entenda seu problema e a necessidade da cria\u00e7\u00e3o de um modelo ou regra de neg\u00f3cio. Pode ser que seja necess\u00e1rio um modelo mais simples, que fique pronto em um menor tempo para gerar valor de forma mais r\u00e1pida, ou pode ser necess\u00e1rio um modelo mais complexo, que ir\u00e1 demandar um tempo maior. Pode ser tamb\u00e9m que para o problema n\u00e3o seja necess\u00e1rio um modelo, e sim um conjunto de regras de neg\u00f3cio a ser aplicado.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#1-definicao-do-target","title":"1 - Defini\u00e7\u00e3o do Target","text":"<p>Nesse momento, juntamente com a \u00e1rea de neg\u00f3cios temos que definir nosso objeto de estudo, juntamente com o que queremos modelar (target).</p> <p>Para a defini\u00e7\u00e3o do target devemos ter um tempo de observabilidade \u00e0 frente, a partir da nossa data de refer\u00eancia, para avaliar o comportamento desses clientes que ser\u00e3o utilizados para fomentar nosso modelo. No caso de trabalhar com modelos de target bin\u00e1rio a defini\u00e7\u00e3o de marca\u00e7\u00e3o a partir do EVER ou OVER vai depender da estrat\u00e9gia buscada. </p>"},{"location":"A_modeling_guideline/modeling_guideline/#ever-ou-over-qual-escolher","title":"Ever ou Over? Qual escolher?","text":"<p>O EVER ser\u00e1 mais conservador, enquanto o OVER ser\u00e1 mais liberal. No caso do EVER, o cliente que atrasa ee paga o juros, voltando a ficar adimplente \u00e9 considerado ruim, pois olhamos os atrasos at\u00e9 a janela final, enquanto para o OVER o cliente que atrasa e volta a pagar antes do final da janela \u00e9 considerado como bom pois olhamos apenas o retrato do final da janela. Para definir se ser\u00e1 usado um ou outro pode depender da forma que as bases da institui\u00e7\u00e3o s\u00e3o constru\u00eddas, por exemplo: clientes que renegociam contratos aparecem com novo contrato sem atraso, se olharmos o EVER teremos o atraso intram\u00eas do cliente, enquanto se olharmos o OVER n\u00e3o teremos a informa\u00e7\u00e3o que ele atrasou e vamos consider\u00e1-lo bom.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#ever","title":"Ever","text":"<pre><code>DT[, mob := min_dt_ref %--% dt_ref %/% months(1)] #define os mobs (a dt_ref minima pode ser a data de ativacao por exemplo no contexto de onboarding)\n\nDT &lt;-  DT[mob &gt;= 0 &amp; mob &lt;= 6 | is.na(mob)] #definido mob 6 caso se trate de um target ever60mob6, sendo essa a data m\u00e1xima avaliada. O NA est\u00e1 incluso em casos que n\u00e3o temos ativacao por exemplo ou clientes que n\u00e3o est\u00e3o na nossa tabela do CADOC\n\nDT_perf &lt;- DT[, .(fl_default = max(as.integer(atraso &gt;= 60)), ##flaga todos os meses em atraso ou n\u00e3o atraso e soma, caso o m\u00e1ximo seja 0 n\u00e3o temos default no intervalo\n             mob_default = min(mob[atraso&gt;=60]), #conta o n\u00famero de mobs, com o m\u00ednimo temos o m\u00eas em que o default ocorreu\n             qtd_meses_materialidade = sum(saldo_devedor&gt;50)), # n\u00fameros de meses que temos materialidade \n            .(cpf_cnpj, safra, qtd_mob)]\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#over","title":"Over","text":"<pre><code>DT[, mob := min_dt_ref %--% dt_ref %/% months(1)] #define os mobs (a dt_ref minima pode ser a data de ativacao por exemplo no contexto de onboarding)\n\nDT &lt;-  DT[mob &gt;= 0 &amp; mob &lt;= 6 | is.na(mob)] #definido mob 6 caso se trate de um target ever60mob6, sendo essa a data m\u00e1xima avaliada. O NA est\u00e1 incluso em casos que n\u00e3o temos ativacao por exemplo ou clientes que n\u00e3o est\u00e3o na nossa tabela do CADOC\n\nDT_perf &lt;- DT[, .(fl_default = max(as.integer(atraso &gt;= 60)), ##flaga todos os meses em atraso ou n\u00e3o atraso e soma, caso o m\u00e1ximo seja 0 n\u00e3o temos default no intervalo\n             mob_default = min(mob[atraso&gt;=60]), #conta o n\u00famero de mobs, com o m\u00ednimo temos o m\u00eas em que o default ocorreu\n             qtd_meses_materialidade = sum(saldo_devedor&gt;50)), # n\u00fameros de meses que temos materialidade \n            .(cpf_cnpj, safra, qtd_mob)]\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#definicao-de-bases","title":"Defini\u00e7\u00e3o de bases","text":"<p>Ap\u00f3s o target definido, deve ser feito um levantamento das bases que podem ser utilizadas para melhor explicar nosso target</p> <ul> <li>Dados internos</li> <li>Dados externos como SCR, Bureaus (Serasa, Quod, Boa vista, Belvo, Neurotech)</li> </ul>"},{"location":"A_modeling_guideline/modeling_guideline/#2-build-features","title":"2 - Build Features","text":"<p>A depender de cada caso a cria\u00e7\u00e3o de features pode ser antes ou depois da defini\u00e7\u00e3o do p\u00fablico eleg\u00edvel</p> <p>A partir do target definido, todo o escopo do projeto entendido, partimos para a cria\u00e7\u00e3o das features</p> <p>Para criarmos as features temos que nos atentar ao horizonte de feature que vamos utilizar. Podemos utilizar 3 meses de features, dessa forma pegamos a partir de uma data de refer\u00eancia, os tr\u00eas meses anteriores para montar nossas features.</p> <ul> <li>Uma op\u00e7\u00e3o \u00e9 juntar todas as bases cruas que ser\u00e3o utilizadas e posteriormente criar nossas features para evitar retrabalho e como melhor pr\u00e1tica</li> <li>Outra op\u00e7\u00e3o \u00e9 criar as features para cada base separadamente pois dessa maneira \u00e9 mais f\u00e1cil identificar poss\u00edveis problemas nas bases ou features criadas, essa op\u00e7\u00e3o se torna vi\u00e1vel quando n\u00e3o se tem dom\u00ednio ou confian\u00e7a suficiente sobre determinada base</li> </ul> <p>Podemos criar features utilizando diferentes horizontes de tempo para os clientes, por\u00e9m, para isso devemos ter alguma forma de identificar os clientes em que nosso tempo de features varia. Por exemplo: Se quisermos pegar como features o hist\u00f3rico de atraso de um cliente, devemos ter tamb\u00e9m uma feature com o tempo de contrato desse cliente, pois \u00e9 muito diferente um cliente que atrasou 2 parcelas em 2 meses com um cliente que atrasou 2 parcelas em 10 anos</p>"},{"location":"A_modeling_guideline/modeling_guideline/#21-publico-elegivel","title":"2.1 P\u00fablico Eleg\u00edvel","text":"<p>Avaliando, dentre toda nossa base, os clientes que possuem o horizonte definido de features, e as demais premissas necess\u00e1rias (hard filters, tempo de observabilidade para o target, etc), estes tornam-se nosso p\u00fablico eleg\u00edvel</p> <p>Tendo a base de features para o p\u00fablico eleg\u00edvel devemos valid\u00e1-la, para isso uma op\u00e7\u00e3o pode ser dividir sua base por meses e avaliar a volumetria por safra, aquelas que aparentarem inconsistentes, podemos dar um double check com uma query na determinada data, por exemplo.</p> <p>\u00c9 importante que a medida que seu p\u00fablico eleg\u00edvel vai se afunilando, voc\u00ea reporte os valores retirados em cada passo.</p> <p>Para a defini\u00e7\u00e3o do p\u00fablico eleg\u00edvel uma forma de tentar garantir a independ\u00eancia da sua base \u00e9 amostrando seu p\u00fablico. Quando voc\u00ea est\u00e1 em uma situa\u00e7\u00e3o em que n\u00e3o possui safra, ou seja, o mesmo cliente pode aparecer diversas vezes (collection, behavior, pre-creli) podemos criar janelas em que a mesma observa\u00e7\u00e3o n\u00e3o possa aparecer at\u00e9 o fim dessa janela, essa op\u00e7\u00e3o permite ainda amostrar dentro de cada m\u00eas para evitar que grande parte do p\u00fablico apare\u00e7a no primeiro m\u00eas e s\u00f3 possa aparecer novamente na pr\u00f3xima janela. Permitindo uma \"melhor aleatoriza\u00e7\u00e3o\" da nossa base.</p> <pre><code>DT_unique &lt;- unique(DT[atraso %in% (30:60)][order(dt_ref)])#aqui temos o p\u00fablico eleg\u00edvel que no caso s\u00e3o clientes com atraso entre 30 e 60\n# Fazer a amostragem ------------------------------------------------------\n\ndata &lt;- unique(DT_unique$dt_ref)\ndata &lt;- data[order(data)]\n\nset.seed(090815)\nall_obs &lt;- DT_unique[dt_ref == data[length(data)],\n                     slice_sample(.SD, n = min(.N, 150))\n] #pega ultimo mes dispon\u00edvel\n\n# Cria a amostragem para os outros meses\nfor ( i in (length(data)-1):1){\n  contrato_excluidos &lt;- all_obs[dt_ref &gt; data[i] &amp; dt_ref &lt; data[i] %m+% months(X), cd_contrato] \n  temp &lt;- base_3040_unique[dt_ref == data[i] &amp; !cd_contrato %in% contrato_excluidos,\n                           slice_sample(.SD, n = min(.N, 150))\n                           ]\n  all_obs &lt;- rbind(all_obs,temp)\n\n}#criamos uma janela do dia mais recente e pegando os meses anteriores respeitando nossa janela de X meses\n</code></pre> <p>Se em nossa situa\u00e7\u00e3o tivermos a safra do cliente (application, onboarding, acesso direto) basta encontrar uma forma de amostrar esses clientes olhando a safra de cada um e o n\u00famero de meses pra frente (para olhar o target) e para tr\u00e1s (para olhar as features) </p> <pre><code>\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#3-analise-exploratoria-univariada-das-variaveis-de-interesse-features-inclusas","title":"3 - An\u00e1lise explorat\u00f3ria univariada das vari\u00e1veis de interesse (features inclusas)","text":"<p>Nesse momento a an\u00e1lise tem como objetivo encontrar outliers, semelhan\u00e7as na distribui\u00e7\u00e3o das features, etc</p> <ul> <li>Um ponto de aten\u00e7\u00e3o s\u00e3o as features com variabilidade pr\u00f3xima de 0, que talvez devam ser manipuladas ou at\u00e9 exclu\u00eddas.</li> <li>Remo\u00e7\u00e3o de features altamente correlacionadas</li> <li>Se o objetivo for a estima\u00e7\u00e3o utilizamos a base full, caso seu objetivo seja de predi\u00e7\u00e3o dividimos a base</li> <li>Esse passo pode tamb\u00e9m acontecer ap\u00f3s a an\u00e1lise bivariada, afim de conhecer melhor a base com features n\u00e3o significativas.</li> <li>Para evitar outliers, uma op\u00e7\u00e3o seria fixar os valores m\u00e1ximos e m\u00ednimos das features a partir dos percentis (alpha)% e (100-alpha)% por exemplo (outras a\u00e7\u00f5es podem ser tomadas, n\u00e3o existe f\u00f3rmula m\u00e1gica)</li> </ul> <p>Para sermos mais conservadores em rela\u00e7\u00e3o ao outliers podemos considerar <code>Outlier extremo</code>: Considerar valores menores que (Q1 - 3*dist AIQ) e maiores que (Q3 + 3*dist AIQ)</p>"},{"location":"A_modeling_guideline/modeling_guideline/#4-analises-bivariadas-para-validacao-das-features-pode-se-utilizar-decision-tree","title":"4 - An\u00e1lises bivariadas para valida\u00e7\u00e3o das features (pode-se utilizar decision tree)","text":"<p>Nesse momento a an\u00e1lise tem como objetivo observar se alguma feature vai de desencontro com nosso target, deixando um ponto de aten\u00e7\u00e3o.</p> <p>\u00c9 importante lembrar que na an\u00e1lise explorat\u00f3ria bivariada, comparando as features com o target, estamos cruzando as informa\u00e7\u00f5es das features com horizonte de X meses para tr\u00e1s, com o target definido com informa\u00e7\u00f5es X meses a frente.</p> <p>Nessa an\u00e1lise \u00e9 interessante observar se a distribui\u00e7\u00e3o das nossas vari\u00e1veis \u00e9 uniforme ao longo do tempo, para isso pode ser feita uma an\u00e1lise de drift m\u00eas a m\u00eas tanto utilizando PSI quanto Wasserstein. Uma outra maneira \u00e9 pegar algumas estat\u00edsticas(m\u00e9dia, mediana...) das vari\u00e1veis e observar se elas variam muito m\u00eas a m\u00eas ou fazer um teste <code>ks.test</code> para avaliar se ambas as amostras vem de uma mesma distribui\u00e7\u00e3o.</p> <p>Caso encontre uma aleatoriedade nas vari\u00e1veis m\u00eas a m\u00eas, dessa forma \u00e9 preciso tentar entender o porque de nossa popula\u00e7\u00e3o mudar tanto a cada safra. Vale uma reflex\u00e3o de caso houver uma cria\u00e7\u00e3o de janela para tentar garantir uma independ\u00eancia entre as observa\u00e7\u00f5es que queremos coletar (exemplo: criamos janelas com espa\u00e7amento de 6 meses em que o cliente s\u00f3 pode aparecer novamente no in\u00edcio da pr\u00f3xima janela para tentar garantir a independ\u00eancia das observa\u00e7\u00f5es que queremos coletar): Se criamos uma janela, o comportamento das nossas vari\u00e1veis pode n\u00e3o ficar bem representado safra a safra, dessa forma pode ser interessante reamostrar a base</p> <p>Essa an\u00e1lise safra a safra pode tamb\u00e9m ser feita na base de Treino, logo ap\u00f3s a separa\u00e7\u00e3o da base entre <code>treino</code>, <code>teste (Out of Sample)</code> e <code>homologa\u00e7\u00e3o (Out of Time)</code>, nesse caso olhamos primeiro a base de treino</p> <p>Na an\u00e1lise explorat\u00f3ria deve ter observado se m\u00eas a m\u00eas as vari\u00e1veis mantem um padr\u00e3o de comportamento</p> <pre><code>#observando a m\u00e9dia\ndist_variaveis &lt;- DT %&gt;% melt(id.vars = \"dt_ref\") %&gt;%\n  .[, mean(value), .(dt_ref, variable)] %&gt;%\n  dcast(dt_ref ~ variable)\n</code></pre> <p>O KS \u00e9 sens\u00edvel ao tamanho de sua amostra, ent\u00e3o amostras muito grandes rejeitam a hip\u00f3tese de igualdade entre a distribui\u00e7\u00e3o das vari\u00e1veis mais facilmente</p> <pre><code>#KS\nfeats &lt;- c(names(model$finalModel$coefficients)[-1])\ndrift &lt;- dados_treino %&gt;% dplyr::select(all_of(feats))\n\ndrift[,data_m0:=dados_treino$dt_mod]\n\nks_stat_drift &lt;- function(m0, var, d){\n  cat(as.character(m0), \":: \", var,\"\\n\")\n  m0 &lt;- ymd(floor_date(m0, unit = \"month\"))\n  mm1 &lt;- m0 %m-% months(1)\n  dm1 &lt;- d[floor_date(data_m0, unit = \"month\") == ymd(mm1), var,with=F] %&gt;% as.data.frame() # == para est\u00e1tico e &lt;= para acumulado\n  d0 &lt;- d[floor_date(data_m0, unit = \"month\") == m0, var, with=F] %&gt;% as.data.frame()\n  #print(paste(m0,'|',var))\n  ks &lt;- ks.test(dm1[,1],d0[,1])\n  z &lt;- t.test(dm1[,1],d0[,1])\n  return(list(var=var,\n              m0=m0,\n              ks_stat=ks$statistic,\n              ks_p_val=ks$`p.value`,\n              t=z$statistic,\n              t_p=z$`p.value`))\n}\n</code></pre> <p>Para diferenciar a m\u00e9trica est\u00e1tica da acumulada safra a safra, opta-se por utilizar <code>dm1</code> e <code>d0</code> com sinais, <code>==</code> para observar a rela\u00e7\u00e3o entre o m\u00eas e o anterior ou <code>&lt;=</code> para observar a rela\u00e7\u00e3o entre o m\u00eas com o acumulado dos demais meses.</p> <p>O Wasserstein mede o trabalho necess\u00e1rio para transformar uma distribui\u00e7\u00e3o em outra a partir dos valores reais das vari\u00e1veis, que podem variar muito e, consequentemente, deixar sua escala muito grande, por essa raz\u00e3o para utilizar o Wasserstein \u00e9 necess\u00e1rio escalar as features. O problema do Wasserstein se deve a falta de no\u00e7\u00e3o relacionado ao valor da escala, n\u00e3o \u00e9 poss\u00edvel saber se em uma escala que varia de 1 a 5, o valor 1 \u00e9 bom ou tudo de 1 a 5 \u00e9 ruim, por exemplo.</p> <pre><code>#Wasserstein\nfeats &lt;- c(names(model$finalModel$coefficients)[-1])\ndrift &lt;- dados_treino %&gt;% dplyr::select(all_of(feats))\n\ndrift[,data_m0:=dados_treino$dt_mod]\n\nwasser_stat_drift &lt;- function(m0,var,d){\n  m0 &lt;- ymd(floor_date(m0, unit = \"month\"))\n  mm1 &lt;- m0 %m-%months(1)\n  dm1 &lt;- d[floor_date(data_m0, unit = \"month\") == ymd(mm1), var, with=F] %&gt;% as.data.frame() # == ou &lt;=\n  d0 &lt;- d[floor_date(data_m0, unit = \"month\") == m0, var, with=F]%&gt;% as.data.frame() # == ou &lt;=\n  print(paste(m0,'|',var))\n  w &lt;- wasserstein1d(dm1[,1],d0[,1],p=1)\n  #z &lt;- t.test(dm1[,1],d0[,1])\n  return(data.frame(var=var,\n                    m0=m0,\n                    wasser=w\n                    #ks_p_val=ks$`p.value`,t=z$statistic,\n                    #t_p=z$`p.value`)\n  ))\n}\n\nfeats &lt;- append(feats, \"data_m0\")\ndados &lt;- base %&gt;% dplyr::select(all_of(feats)) %&gt;% mutate_if(is.numeric, scale) \n\n</code></pre> <p>O PSI assim como o Wasserstein n\u00e3o \u00e9 sens\u00edvel ao tamanho da amostra, por\u00e9m em seu processo s\u00e3o separadas as vari\u00e1veis em grupos e comparados, dessa forma o PSI \u00e9 sens\u00edvel ao n\u00famero de bins (grupos)</p> <pre><code>#PSI\nfeats &lt;- mf2$feature_names\n\ndrift &lt;- treino3 %&gt;% select(any_of(feats))\n\ndrift[,data_m0:=treino3$data_m0]\ndrift[,m0:=NULL]\n\nks_stat_drift &lt;- function(m0,var,d){\n  m0 &lt;- ymd(floor_date(m0, unit = \"month\"))\n  mm1 &lt;- m0 %m-% months(1)\n  dm1 &lt;- d[floor_date(data_m0, unit = \"month\") &lt;= mm1,var,with=F] %&gt;% as.data.frame()\n  d0 &lt;- d[floor_date(data_m0, unit = \"month\") == m0,var,with=F]%&gt;% as.data.frame()\n  #print(paste(m0,'|',var))\n  ks &lt;- ks.test(dm1[,1],d0[,1])\n  z &lt;- t.test(dm1[,1],d0[,1])\n  dm1$m &lt;- mm1\n  d0$m &lt;- m0\n  psi &lt;- get_psi(dat=rbind(dm1,d0),occur_time='m')\n  return(data.frame(var=var,m0=m0,ks_stat=ks$statistic,ks_p_val=ks$`p.value`,t=z$statistic,\n         t_p=z$`p.value`,psi=psi[,unique(PSI)]))\n\n}\n</code></pre> <p>Ap\u00f3s escolher qual fun\u00e7\u00e3o utilizar \u00e9 necess\u00e1rio rodar o <code>map</code> e posteriormente o ggplot caso queira visualizar o gr\u00e1fico do drift</p> <pre><code>x &lt;- map2_dfr(.x = rep(seq.Date(ymd('2018-05-01'), #primeiro mes da sua base +1\n                                ymd('2022-06-01'), #\u00faltimo mes da sua base\n                                by = 'month'),\n                       each = 17), #quantidade de vari\u00e1veis\n              .y = rep(names(drift), 50), #quantidade de meses entre o in\u00edcio e fim das datas\n              .f = ~ ks_stat_drift(.x, .y, base)) %&gt;% setDT()\n\nx[, sig:=as.integer(ks_p_val&lt;=0.05)]\n\nx2 &lt;- data.table::dcast(x,var~m0,value.var = 'ks_stat')\n\n#fun\u00e7\u00e3o para plotar o gr\u00e1fico\nggplot(x,aes(x=m0,y=var,fill=ks_stat))+\n  geom_tile(width=50,col='white')+\n  scale_x_date(date_labels = \"%m/%Y\",date_breaks = \"29 day\",expand = c(0,0))+\n  # scale_x_date(expand = c(0,0))+\n  labs(x=\"\",y=\"\",title = \"Drift ao longo do tempo - Wasserstein m\u00eas anterior\",\n       fill = \"\")+\n  theme_minimal()+\n  scale_fill_distiller(type='div')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#5-regression","title":"5 - Regression","text":"<p>A regress\u00e3o, por sua simplicidade, deve ser sempre a primeira escolha na hora de criar seu modelo baseline. Ap\u00f3s um primeiro teste podemos partir para outros modelos mais robustos.</p> <ul> <li> <p>Devemos interpretar os coeficientes e suas dire\u00e7\u00f5es e ver se fazem sentido para o contexto estudado</p> </li> <li> <p>Escalar as vari\u00e1veis e retreinar o modelo. Como a regress\u00e3o log\u00edstica \u00e9 muito sens\u00edvel a outliers coisas muito estranhas, como KS variando muito safra a safra ou coeficiente que esperasse ser significativo n\u00e3o sendo, podem ser explicados por n\u00e3o escalar as vari\u00e1veis.</p> </li> </ul> <p>Em alguns casos ter o trabalho de escalar as vari\u00e1veis pode ser em v\u00e3o, como por exemplo em casos que temos padr\u00f5es muito diferentes de nossas features safra a safra, com comportamentos completamente aleat\u00f3rios para uma mesma vari\u00e1vel.</p> <p>Uma alternativa para a regress\u00e3o linear, por exemplo, em que as vari\u00e1veis n\u00e3o s\u00e3o significativas, \u00e9 utilizar <code>GAMs</code> para testar se as vari\u00e1veis tem uma rela\u00e7\u00e3o n\u00e3o linear, de maneira simples.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#51-machine-learning","title":"5.1 - Machine Learning","text":"<p>Outra possibilidade \u00e9 utilizar modelos mais robustos como por exemplo XGBoost, que nada mais \u00e9 que um conjunto de \u00e1rvores, com seus par\u00e2metros bem tunados ou LightGBM que \u00e9 uma vers\u00e3o mais r\u00e1pida do XGBoost.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#tunando-parametros","title":"Tunando par\u00e2metros","text":"<p>Esse passo \u00e9 essencial para encontrar qual o melhor conjunto de par\u00e2metros para obter uma melhor performance para seu modelo</p> <pre><code>tune_grid_xgb&lt;- expand.grid(\n    nrounds= c(50,100),\n    eta = c(0.02, 0.01), #taxa de aprendizado, quanto menor, menor a chance de overfitting\n    gamma = c(1), #quanto maior mais conservador o algoritmo [0,infinito]\n    max_depth = 4:6, #profundidade da \u00e1rvore(complexidade) de [0,infinito] sendo 0 sem limita\u00e7\u00e3o de profundidade\n    min_child_weight = c(5),\n    subsample= c(0.5),#propor\u00e7\u00e3o de subamostra de treinamento, 50% dos dados de treino ser\u00e3o amostrados antes da cria\u00e7\u00e3o das arvores\n    colsample_bytree=0.2 #propor\u00e7\u00e3o das vari\u00e1veis escolhidas aleatoriamente para contruir cada \u00e1rvore\n  )\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#interpretacao","title":"Interpreta\u00e7\u00e3o","text":"<p>Em modelos mais robustos, assim como na regress\u00e3o, existe a necessidade de interpreta\u00e7\u00e3o. Enquanto em um basta interpretar os coeficientes e observar se o sentido de cada um faz sentido para o contexto estudado, para esses modelos \"Black Box\" temos o SHAP(SHapley Additive exPlanations) para permitir a interpretabilidade das vari\u00e1veis. Utilizando o SHAP na nossa base de treino conseguimos tirar insights sobre o que o modelo aprender com cada uma das vari\u00e1veis dispostas.</p> <p>Para utilizar o SHAP, caso estejamos com um modelo xgboost precisamos primeiro trasformar nossa base em uma matriz para posteriormente utilizarmos a fun\u00e7\u00e3o, como podemos ver abaixo:</p> <pre><code>pred_data &lt;- model.matrix(.outcome~., model$trainingData)[,-1]#passando uma matriz(caso o modelo seja um xgboost)\nexplain(model$finalModel, exact = TRUE, newdata = pred_data) %&gt;% setDT() #fun\u00e7\u00e3o para retornar shap value, caso o modelo seja um xgboost \u00e9 necess\u00e1rio passar uma matriz\n\n</code></pre> <p>Ap\u00f3s a an\u00e1lise do SHAP, caso tenha muitas features, na hora de apresentar a relev\u00e2ncia das vari\u00e1veis pode ser interessante utilizar o detalhamento de Pareto para mostrar apenas os \"20% de features que explicam 80% do modelo\"</p>"},{"location":"A_modeling_guideline/modeling_guideline/#6-modelagem","title":"6 - Modelagem","text":""},{"location":"A_modeling_guideline/modeling_guideline/#splitting","title":"Splitting","text":"<ul> <li> <p>Dividir nossa base em treino, teste (Out of Sample) e homologa\u00e7\u00e3o (Out Of Time)</p> </li> <li> <p>Homologa\u00e7\u00e3o (OOT): pegamos os meses mais recentes para conseguirmos uma valida\u00e7\u00e3o mais quente da performance do nosso modelo. A homologa\u00e7\u00e3o s\u00f3 pode ser testada uma \u00fanica vez, ap\u00f3s todo o tr\u00e2mite com a base de treino e teste. Utilizar a base de homologa\u00e7\u00e3o mais de uma vez \u00e9 incorreto pois voc\u00ea estar\u00e1 balizando seu resultado mais recente para definir se seu modelo est\u00e1 bom ou n\u00e3o, quando na verdade ele deve servir apenas para validar se o que foi feito com o treino e teste estava bom. Via de regra, se tudo estiver ok com o treino/teste a homologa\u00e7\u00e3o ser\u00e1 um reflexo disso. O que n\u00e3o siginifca que caso o OOT estiveja ruim, tenhamos necessariamente um problema como o modelo, pode ser que a popula\u00e7\u00e3o de treino j\u00e1 n\u00e3o \u00e9 mais reflexo da popula\u00e7\u00e3o atual, ou que tenhamos uma sazonalidade no per\u00edodo do OOT analisado, ou que existam padr\u00f5es temporais que n\u00e3o conseguimos captar como o modelo, ou que passou despercebido pelo analista.</p> </li> </ul> <p>Do restante da base dividimos em 2 grupos seguindo a l\u00f3gico do 70/30 ou outras, como por exemplo considerar um erro fixo para treino e teste e a partir dai definir o tamanho ideal das amostras</p> <ul> <li>Treino: selecionando 70% da base restante</li> <li>Teste: selecionando 30% da base restante</li> </ul> <pre><code>#Op\u00e7\u00e3o 1\nsplit_test &lt;- function(z = 1.96, e = 0.025, sd = 0.5){\n  n_teste = ((z*sd)/e)^2\n  n_teste\n} #fun\u00e7\u00e3o para encontrar o tamanho da amostra de teste fixando o erro do treino e teste\n#depende indiretamente do tamanho da base pois e = z*(sigma/sqrt(n))\n\n#Op\u00e7\u00e3o 2\nsplit_test &lt;- function(base){\n  n &lt;- nrow(base) #tamanho populacao\n  z &lt;- 1.96 #z-score\n  e &lt;- 0.025#margem de erro\n  sd &lt;- 0.5 #desvio padr\u00e3o\n\n  (((z^2)*sd*(1-sd))/e^2)/1+(((z^2)*sd*(1-sd))/(e^2)/n)\n}#fun\u00e7\u00e3o para encontrar o tamanho da amostra de teste fixando o erro do treino e teste\n\n</code></pre> <p>Ap\u00f3s a separa\u00e7\u00e3o das amostras devemos fazer um teste PSI (Population Stability Index) para avaliar se nossa Homologa\u00e7\u00e3o, Treino e Teste est\u00e3o semelhantes. Como no OOT n\u00e3o existe amostragem pode ser que ele se distingua muito da sua base de treino e teste. Bastaria fazer a compara\u00e7\u00e3o de Homologa\u00e7\u00e3o com o Treino ou o Teste, por\u00e9m, para uma maior certeza da aleatoriedade da fun\u00e7\u00e3o do R fazemos a compara\u00e7\u00e3o tanto com o Treino quanto com o Teste apenas como um double check.</p> <p>Para testar o PSI podemos utilizar pacotes como <code>creditmodel</code> e <code>PDtoolkit</code></p> <p>Al\u00e9m do PSI \u00e9 interessante fazer uma an\u00e1lise de drift para avaliar se features das amostras de treino, teste e homologa\u00e7\u00e3o est\u00e3o bem representadas.</p> <p>Na an\u00e1lise explorat\u00f3ria deve ter observado se m\u00eas a m\u00eas as vari\u00e1veis mantem um padr\u00e3o de comportamento</p> <p>De in\u00edcio, testamos um modelo mais simples para avaliar a performance do nosso baseline, em um segundo momento vale a pena compararmos a performance do Random Forest, XGboost, dentre diversos outros poss\u00edveis.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#modeling","title":"Modeling","text":""},{"location":"A_modeling_guideline/modeling_guideline/#full-model","title":"Full model","text":"<p>Primeiro devemos transformar nosso target em fator e em seguida podemos rodar um primeiro modelo com todas as vari\u00e1veis (modelo full).</p> <p>Ap\u00f3s cada passo a seguir podemos rodar um modelo para ir acompanhando a performance do modelo atrav\u00e9s do KS</p> <p>Utilizamos a fun\u00e7\u00e3o abaixo para retornar um vetor com a probabilidade de cada vari\u00e1vel ser considerada 1 no nosso target</p> <pre><code>probabilities &lt;- model %&gt;% predict(teste, type = \"response\")\n</code></pre> <p>Caso possua um valor de refer\u00eancia para avaliar a precis\u00e3o do modelo (por exemplo 0.8), o c\u00f3digo abaixo pode ser \u00fatil</p> <pre><code>### predi\u00e7\u00e3o ----\npredicted.classes &lt;- ifelse(probabilities &gt; 0.8, \"PG\", \"NPG\")\n# precisao do modelo\nmean(predicted.classes == teste$fl_good)\n</code></pre> <p>Para calcular o KS da base de treino utilizamos:</p> <pre><code>### KS treino ----\nsetDT(treino)\ntreino[, fl_good := ifelse(target == \"NPG\", 0, 1)]#transformando seu target em num\u00e9rico para medir o KS\nMLmetrics::KS_Stat(y_pred = model$fitted.values, y_true = treino$target)\n</code></pre> <p>Para calcular o KS da base de teste utilizamos:</p> <pre><code>### KS teste ----\nsetDT(teste)\nteste[, target := ifelse(target == \"NPG\", 0, 1)]#transformando seu target em num\u00e9rico para medir o KS\nMLmetrics::KS_Stat(y_pred = probabilities, y_true = teste$target)\n\n</code></pre> <p>Ap\u00f3s calcular o KS vai ser interessante olhar a curva ROC e o valor do AUC e PRAUC que seu modelo retorna.</p> <ul> <li>Vale resaltar que o AUC independe de escala pois n\u00e3o utiliza valores absolutos e sim a precis\u00e3o das classifica\u00e7\u00f5es</li> <li>O AUC n\u00e3o \u00e9 uma boa m\u00e9trica para se observado em uma base desbalanceada, pois como avalia apenas a taxa de verdadeiro positivo e falso positivo ela facilmente conseguir\u00e1 um alto AUC caso tenha muitos TRUE e baixo caso tenha muitos FALSE</li> <li>O PRAUC \u00e9 utilizado em bases desbalanceadas</li> </ul> <p>Precision: \\(\\frac{True Positive}{True Positive + False Positive}\\)   Recall: \\(\\frac{True Positive}{True Positive + False Negative}\\)</p> <p>Bases desbalanceadas facilmente conseguem um AUC alto</p> <pre><code>roc1=plot.roc(treino$target,fitted(model))\n\nplot(roc1,\n     print.auc=TRUE,\n     auc.polygon=TRUE,\n     grud=c(0.1,0.2),\n     max.auc.polygon=TRUE,\n     auc.polygon.col=\"#ff7a00\",\n     print.thres=TRUE)\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#filtered-models","title":"Filtered models","text":""},{"location":"A_modeling_guideline/modeling_guideline/#without-zerovar-features","title":"without ZeroVar features","text":"<p>Em seguida podemos avaliar se temos vari\u00e1veis sem vari\u00e2ncia utilizando <code>nearZeroVar(DT,freqCut=98/2, saveMetrics= TRUE)</code> e rodar um novo modelo seguindo os passos de modelagem anteriores. A cada novo passo adicione as vari\u00e1veis que v\u00e3o sair do seu modelo em um vetor <code>cols_ignore &lt;- append(cols_ignore, rownames(var_nearzerovar)[var_nearzerovar$nzv])</code></p>"},{"location":"A_modeling_guideline/modeling_guideline/#with-little-correlation","title":"with little correlation","text":"<p>Depois de avaliar se temos vari\u00e1veis sem vari\u00e2ncia podemos buscar por correla\u00e7\u00e3o entre as restantes com <code>findCorrelation(DT %&gt;% select_if(is.numeric) %&gt;% cor(), cutoff = .75)</code></p> <p>Note que utilizamos apenas vari\u00e1veis num\u00e9ricas pois para outras vari\u00e1veis n\u00e3o faz sentido olhar correla\u00e7\u00e3o dessa forma</p> <p>Adicione as novas vari\u00e1veis no seu vetor de vari\u00e1veis a serem ignoradas <code>cols_ignore &lt;- append(cols_ignore, var_corr)</code> e rode um novo modelo para continuar acompanhando o desempenho do modelo com o <code>MLmetrics::KS_Stat(y_pred = probabilities, y_true = teste$target)</code></p>"},{"location":"A_modeling_guideline/modeling_guideline/#with-little-vif","title":"with little VIF","text":"<p>Para esse passo retiramos vari\u00e1veis que tenham um VIF alto, na literatura VIF &gt; 5 \u00e9 considerado alto, por\u00e9m para sermos mais conservadores podemos considerar VIF &gt; 10. Esse passo serve como um double-check da an\u00e1lise de correla\u00e7\u00e3o feita anteriormente, para garantir que as vari\u00e1veis n\u00e3o est\u00e3o correlacionadas, ent\u00e3o \u00e9 bem comum que nesse passo n\u00e3o encontre vari\u00e1veis para serem retiradas caso tenha considerado um cut razo\u00e1vel para seu findCorrelation (\\&lt; 0.7 por exemplo)</p> <pre><code>vif_model &lt;- vif(model)\nvif_model &lt;- as.data.frame(vif(model))\ncols_ignore &lt;- append(cols_ignore, rownames(vif_model)[vif_model &gt; 10])\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#value-of-the-information","title":"Value of the Information","text":""},{"location":"A_modeling_guideline/modeling_guideline/#information-value","title":"Information Value","text":"<p>Ap\u00f3s os passos anteriores, rodamos um Information Value afim de observar se existe alguma rela\u00e7\u00e3o linear univariada entre alguma feature e o target.</p> <p>No caso em que voc\u00ea tenha muitas features e o calculo da correla\u00e7\u00e3o estiver demorando muito, \u00e9 poss\u00edvel utilizar o Information Value com um valor baixo (0.02 por exemplo) antes do calculo da correla\u00e7\u00e3o. Mesmo que ele far\u00e1 a an\u00e1lise univariada, e dessa forma j\u00e1 reduzir\u00e1 dimensionalidade do seu banco total para na hora de rodar a correla\u00e7\u00e3o tenhamos menos vari\u00e1veis e o processo seja menos custoso.</p> <p>Rules related from Siddiqi (2006)</p> <p>Not useful for prediction: &lt; 0.02 Weak predictive Power: 0.02 to 0.1 Medium predictive Power: 0.1 to 0.3 Strong predictive Power: 0.3 to 0.5 Suspicious Predictive Power &gt; 0.5</p> <p>Limita\u00e7\u00f5es:</p> <ul> <li>O IV assume uma rela\u00e7\u00e3o linear entre as categorias, dessa forma caso essa rela\u00e7\u00e3o n\u00e3o seja linear a import\u00e2ncia da vari\u00e1vel estar\u00e1 incorretamente calculada.</li> <li>N\u00e3o considera correla\u00e7\u00e3o: O IV trata cada vari\u00e1vel individualmente, n\u00e3o considerando poss\u00edveis itera\u00e7\u00f5es entre vari\u00e1veis. Dessa for, n\u00e3o captura totalmente a complexidade a qual v\u00e1rias vari\u00e1veis podem influenciar o resultado desejado em conjunto.</li> <li>\u00c9 sens\u00edvel ao n\u00famero de <code>bins</code>, pois categorias com baixa volumetria podem n\u00e3o ser significativamente grande para termos uma propor\u00e7\u00e3o real de bons e maus</li> <li>\u00c9 mais apropriado o uso em contextos de classifica\u00e7\u00e3o bin\u00e1ria</li> </ul> <pre><code>IV &lt;- Information::create_infotables(data = DT[, c(vars, 'target'), with = F], y = 'target')$Summary %&gt;% setDT()\n</code></pre> \\[WOE = ln(Good Distribution / Bad Distribuition)\\] \\[IV = \\sum(Good Distribution - BadDistribution)*WOE\\] <p>WOE avalia o poder de previs\u00e3o de uma categoria em rela\u00e7\u00e3o a outra. Se uma categoria apresenta um valor alto de WOE significa que a categoria distingue eficientemente os eventos dos n\u00e3o eventos</p>"},{"location":"A_modeling_guideline/modeling_guideline/#mutual-information","title":"Mutual Information","text":"<p>Ap\u00f3s os passos anteriores, rodamos um Mutual Information afim de observar se existe alguma rela\u00e7\u00e3o n\u00e3o linear entre alguma feature e nosso target.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#stepwise","title":"Stepwise","text":"<p>Utilizar caso precise avaliar o melhor conjunto de features para sua modelagem.</p> <pre><code>RcmdrMisc::stepwise(model, direction = 'backward/forward', criterion = 'BIC')\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#feature-selection-methods","title":"Feature Selection Methods","text":"<p>Nessa etapa final de Feature Engineering podemos utilizar m\u00e9todos como <code>Boruta</code>, <code>infgain</code>, <code>mrmr</code>, dentre outros, para um passo extra de sele\u00e7\u00e3o de features.</p> <ul> <li>Boruta: cria uma vari\u00e1vel aleat\u00f3ria e desconsidera todas as existentes que explicarem menos que a vari\u00e1vel aleat\u00f3ria gerada.</li> <li>InfGain: entropy of the class distribution before and after the split resultando em algo entre 0 (sem ganho) e 1 (ganho m\u00e1ximo)</li> <li> <p>mrmr: maximum Relevancy Minimum Redundancy</p> </li> <li> <p>Lasso</p> </li> </ul> <pre><code>Boruta::Boruta(DT, target~.)\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#teste-hosmer-e-lemeshow","title":"Teste Hosmer e Lemeshow","text":"<p>Utilizamos o teste de Hosmer e Lemeshow (Teste Qui-quadrado) para demonstrar a qualidade do ajuste de nosso modelo de regress\u00e3o log\u00edstica, ou seja, se o modelo pode explicar os dados observados. A hip\u00f3tese nula do teste \u00e9 a de as propor\u00e7\u00f5es observadas e esperadas serem as mesmas ao longo da amostra (alto poder preditiva)</p> \\[H_0$: $Prop_{o_i}$ = $Prop_{e_i}\\] <pre><code>hl = ResourceSelection::hoslem.test(treino$target,fitted(modelo), g = 10)\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#matriz-de-confusao","title":"Matriz de Confus\u00e3o","text":"<p>A matriz de confus\u00e3o te retornar\u00e1 a Sensibilidade, Especificidade, dentre outros indicadores para avaliar a acertividade do seu modelo</p> <p>Para a cria\u00e7\u00e3o dessa matriz, temos que sugerir um ponto de corte, por exemplo, no caso de uma modelagem de pagamento temos que definir um ponto de corte em que dizemos se aquele cliente vai ou n\u00e3o nos pagar. Vimos isso no t\u00f3pico de modelagem para medir a precis\u00e3o do modelo. Nesse contexto agora, n\u00e3o temos esse valor de refer\u00eancia, ent\u00e3o queremos encontr\u00e1-lo, e para isso temos algumas possibilidades:</p> <p>1- Olhar o ponto de corte do KS para o score</p> <pre><code>pred &lt;- ROCR::prediction(predictions = dados_teste$pred, labels = dados_teste$true)\nperf &lt;- ROCR::performance(pred, \"tpr\", \"fpr\")\nTPR &lt;- unlist(perf@y.values)\nFPR &lt;- unlist(perf@x.values)\ndiff_TPRFPR &lt;- TPR - FPR\nmax(diff_TPRFPR)#KS\ncutoffAtKS &lt;- unlist(perf@alpha.values)[which.max(diff_TPRFPR)][]#corte do Score\n</code></pre> <p>2 - Olhar qual ponto de corte maximiza o F1</p> <pre><code>\n</code></pre> <p>3 - Olhar o ponto de inflex\u00e3o da curva ROC</p> <pre><code>\n</code></pre> <p>Como ambos (KS e F1) s\u00e3o medidas de qualidade geral do modelo, esperasse que os valores encontrados para cada um estejam pr\u00f3ximos</p>"},{"location":"A_modeling_guideline/modeling_guideline/#ks","title":"KS","text":"<p>Um KS muito alto \u00e9 um ponto de aten\u00e7\u00e3o, pode ser que uma vari\u00e1vel foi coletada incorretamente, existe alguma vari\u00e1vel correlacionada com sua resposta, dentre outros. Por exemplo, um KS de 60 em um contexto real, utilizando uma regress\u00e3o \u00e9 algo para se colocar em alerta.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#ks-por-safra","title":"KS por safra","text":"<p>Avaliar o KS por safras para avaliar o desempenho do seu modelo safra a safra. Caso utilize m\u00eas a m\u00eas e sua volumetria seja baixa, seu KS pode ficar bastante vol\u00e1til. Uma maneira de validar essa hip\u00f3tese \u00e9 olhar a distribui\u00e7\u00e3o do KS em intervalos de tempo maiores para aumentar a volumetria, por exemplo bimestral ou semestralmente.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#intervalo-de-confianca-para-o-ks","title":"Intervalo de Confian\u00e7a para o KS","text":"<p>Rodar um bootstrap para calcular o intervalo de confian\u00e7a do nosso KS. Calcular tamb\u00e9m o IC do KS safra a safra, para observarmos se os limites s\u00e3o respeitados.</p> <pre><code>IC_KS = function(data, B, safra){\n  ks_interv &lt;- map_dbl(1:B,~{\n    idx &lt;- sample(dim(data)[1],replace = T)\n    bd &lt;- data[idx]\n    inter.ai::ks_calc(bd[dt_ref == safra, pred],bd[dt_ref == safra, fl_good])\n  }, .progress = TRUE\n  )\n  summary(ks_interv)\n  hist(ks_interv, probability = TRUE)\n  quantile(ks_interv,c(.025,.975))\n}#fun\u00e7\u00e3o retorna o IC do KS do seu modelo a partir de um bootstrap\n#pega a media (ou mediana por a distribui\u00e7\u00e3o do KS ser sim\u00e9trica)\n\nic_ks_safra &lt;- map_dfr(.x = seq.Date(ymd('2018-04-01'),\n                                     ymd('2018-07-01'),\n                                     by = 'months'),\n                       .f = ~ IC_KS(safra = .x, B = 5000, data = treino)) %&gt;% setDT()#fun\u00e7\u00e3o retorna KS safra a safra\n\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#modelo-aleatorio","title":"Modelo Aleat\u00f3rio","text":"<p>Pode acontecer de o modelo estar aparentemente bom, por\u00e9m temos apenas uma correla\u00e7\u00e3o esp\u00faria entre nossas vari\u00e1veis, para isso podemos utilizar um bootstrap (caso nossa amostra seja pequena) e rodar um modelo somente com intercepto e <code>rnorm</code> e avaliar o KS desse modelo safra a safra. Esse modelo necessariamente deve ser ruim, pois uma vari\u00e1vel completamente aleat\u00f3ria n\u00e3o pode explicar bem seu modelo.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#oversampling","title":"Oversampling","text":"<p>Para caso de baixa volumetria, uma tentativa v\u00e1lida \u00e9 utilizar a t\u00e9cnica de Oversampling para aumentar sua amostragem. (Existem muitas cr\u00edticas a essa metodologia, vale a pena estudar mais sobre o assunto)</p> <p>A aus\u00eancia de informa\u00e7\u00e3o tamb\u00e9m \u00e9 informa\u00e7\u00e3o. Treinar um modelo que seja desbalanceado \u00e9 ser fided\u00edgno a realidade do seu problema.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#cross-validate","title":"Cross-Validate","text":"<p>Se estiver cogitando utilizar CV deveria responder tr\u00eas perguntas:</p> <ul> <li>What problem am I trying to solve by using k-fold cross validation?</li> <li>How will I know if k-fold cross validation has solved that problem?</li> <li>Will a different approach be more effective at solving my problem?</li> </ul> <p>Caso algum fold tenha uma discrep\u00e2ncia no KS \u00e9 sinal que temos problemas na base. Um valor baixa de kappa tamb\u00e9m \u00e9 um forte ind\u00edcio que o modelo n\u00e3o est\u00e1 bom.</p> <p>Algo interessante a se observar \u00e9 a compara\u00e7\u00e3o entre o <code>model$pred</code> e o <code>predict(model, type = \"prob)</code> quando se utiliza o m\u00e9todo de cross-validate em seu modelo no caret. Esses valores v\u00e3o ser diferentes pois com o cross-validate temos uma base de treino um pouco diferente da base original.</p> <p>Caso seu modelo esteja com overfitting voc\u00ea ter\u00e1 um valor muito discrepante entre ambos, mas caso os valores estejam pr\u00f3ximos n\u00e3o significa que n\u00e3o temos overfitting</p> <p>Em um contexto geral ap\u00f3s dividir sua base em treino e teste utiliza o CV em sua base de treino para tunar seus hiperpar\u00e2metros, de modo que internamente sua base de treino ser\u00e1 redividida em treino e teste na propor\u00e7\u00e3o de <code>100 - (100/k-folds)</code> para treino e <code>100/k-folds</code> para teste <code>k-folds</code> vezes, definindo entre os <code>k-folds</code> cen\u00e1rios testados qual teve as melhores estimativas de desempenho com maior grau de precis\u00e3o. O benef\u00edcio de utilizar esse m\u00e9todo \u00e9 reduzir a varia\u00e7\u00e3o de nossa estimativa para desempenho fora da amostra (base de treino), n\u00e3o de fato melhorando nosso modelo. Posteriormente ao aplicar o melhor conjunto de hiperpar\u00e2metros para nossa base de teste original n\u00e3o estaremos tendo um problema de overfitting, pois a nossa base de teste n\u00e3o foi utilizada para essa defini\u00e7\u00e3o.</p> <p>Se temos por exemplo 5 folds, a valida\u00e7\u00e3o cruzada pelo caret ir\u00e1 selecionar o fold com os melhores hiperpar\u00e2metros e aplicar na base original de treino. Uma alternativa a se pensar seria utilizar como base de treino o modelo treinado pelo melhor fold, dessa forma podendo dar um perdict no que antes era sua base original de treino, utilizando o predict de teste interno dos demais 4 folds como insumo para a \u00e1rea ne neg\u00f3cio. Dessa forma seria poss\u00edvel dar um predict na base que originariamente era de treino para enviar a \u00e1rea de neg\u00f3cios, diferentemente de se tivessemos treinado o modelo com a base original de treino. \u00c9 importante lembrar que dar um predict na abse que voc\u00ea utilizou para treinar seu modelo resultar\u00e1 em dados excessivamente otimistas que dificilmente ser\u00e3o generalizados para novos dados</p> <p>O Nested Cross Validate \u00e9 utilizado quando originalmente n\u00e3o \u00e9 separada a base em treino e teste, pois internamente j\u00e1 ser\u00e1 separada em treino, teste e valida\u00e7\u00e3o e ser\u00e3o testados os hiperpar\u00e2metros da mesma forma que vimos anteriormente</p> <pre><code># WIP - codigo nao roda e n\u00e3o foi validado\nset.seed(998)\n\nfolds &lt;- createMultiFolds(iris$Species, k = 5, times = 2) ## 2 rep 5-fold CV for performance evaluation\n\ninner_res &lt;- trainControl(method = \"cv\", number = 5) ## 5-fold CV for hyperparameter tuning\n\nmod_list &lt;- lapply(folds, function(x) { # train models for each of the 10 training sets\n  train(Species ~ ., data = iris[x,], \n        method = \"rf\", \n        trControl = inner_res,\n        verbose = FALSE)\n})\n\npred_list &lt;- mapply(function(x, y) { # compute predictions on each of the 100 test sets\n  predict(x, iris[-y,])}, \n  x = mod_list, y = folds)\n\nperf_sens_spec_list &lt;- mapply(function(x, y) { # calculate performance measures I\n  confusionMatrix(x, iris[-y, \"Species\", drop = TRUE])$byClass[c(1,2)]}, \n  x = pred_list, y = folds)\n\nperf_accuracy_kappa_list &lt;- mapply(function(x, y) { # calculate performance measures II\n  confusionMatrix(x, iris[-y, \"Species\", drop = TRUE])$overall[c(1,2)]}, \n  x = pred_list, y = folds)\n\n# average performance across the 100 test sets\nmean(perf_sens_spec_list[\"Sensitivity\", ])\nmean(perf_sens_spec_list[\"Specificity\", ])\nmean(perf_accuracy_kappa_list[\"Accuracy\", ])\nmean(perf_accuracy_kappa_list[\"Kappa\", ])\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#importancia-e-comportamento-da-feature","title":"Import\u00e2ncia e Comportamento da feature","text":"<p>Um ponto interessante de se observar \u00e9 a import\u00e2ncia que a feature tem no seu modelo. Quando estamos trabalhando com modelos de Machine Learning n\u00e3o temos de cara o valor dos nossos coeficientes como temos na regress\u00e3o. Para contornar isso utilizamos a t\u00e9cnica de SHAP, que por meio do shapley value, permite uma interpreta\u00e7\u00e3o do nosso modelo, retornando um coeficiente para cada feature.</p> <pre><code>library(fastshap)\nlibrary(shapviz)\n\ndados_treino1 &lt;- model$finalModel$data\nvars_model &lt;- model$finalModel$xNames\n\nshap &lt;- fastshap::explain(model$finalModel, exact = TRUE, new_data = model$finalModel$xNames)\n\nshap_plot &lt;- shapviz(object = shap, X = dados_treino1 %&gt;% select(all_of(vars_model)))\n\nsv_importance(shap_plot, max_display = 10,kind = \"bar\", fill = \"#ff7a00\") +\n  labs(title = \"SHAP Feature Importance\")\n</code></pre> <p>Al\u00e9m da import\u00e2ncia conseguimos tamb\u00e9m analisar o comportamento das features</p> <pre><code>sv_dependence(shap_plot, v = \"vl_last_atraso\")# feature \u00fanica\n\n#SHAP Beeswarm (Consist\u00eancia :: Sensibility &amp; Sanity)\nsv_importance(shap_plot, kind = \"beeswarm\", size = 0.5) +\n  labs(title = \"SHAP Beeswarm\")\n</code></pre> <p>Podemos analisar feature a feature com esse conjunto de 4 gr\u00e1ficos:</p> <pre><code>lucyR::feature_sanity()\n</code></pre> <p>ressaltando que para vari\u00e1veis cont\u00ednuas o scatterplot \u00e9 utilizado enquanto para vari\u00e1veis discretas utilizamos o boxplot</p>"},{"location":"A_modeling_guideline/modeling_guideline/#61-comparacao-de-modelos","title":"6.1 Compara\u00e7\u00e3o de modelos","text":"<p>Para comparar modelos, com objetivo inferencial, podemos utilizamos o BIC, que \u00e9 um crit\u00e9rio de sele\u00e7\u00e3o de modelos, que penaliza a verossimilhan\u00e7a pelo n\u00famero de par\u00e2metros do modelo proposto. Por\u00e9m em grande parte dos nosso problemas, queremos fazer previs\u00f5es, por esse motivo n\u00e3o o utilizamos.</p> <p>Bayes Factor \u00e9 uma op\u00e7\u00e3o de crit\u00e9rio</p>"},{"location":"A_modeling_guideline/modeling_guideline/#analise-de-drift","title":"An\u00e1lise de Drift","text":""},{"location":"A_modeling_guideline/modeling_guideline/#kl-divergence","title":"KL Divergence","text":"<p>Sens\u00edvel a pequenas mudan\u00e7as, especialmente probabilidades pr\u00f3ximas de zero, levando a grandes flutua\u00e7\u00f5es pois penaliza \u00e1reas com probabilidade baixa ou pr\u00f3xima de zero.</p> <p>Depende da sobreposi\u00e7\u00e3o entre distribui\u00e7\u00f5es, ou seja, \u00e0 medida que os dados de produ\u00e7\u00e3o se afastam dos dados de refer\u00eancia, a falta de sobreposi\u00e7\u00e3o aumenta a volatilidade da m\u00e9trica</p> <p>\u00c9 altamente responsiva a outliers, pois mudan\u00e7as nas caudas da distribui\u00e7\u00e3o podem levar a picos ou quedas repentinas nos valores, tornando-a mais inst\u00e1vel na presen\u00e7a de outliers</p>"},{"location":"A_modeling_guideline/modeling_guideline/#wasserstein","title":"Wasserstein","text":"<p>Mede o 'custo' cumulativo para transformar uma distribui\u00e7\u00e3o em outra, com foco nas dist\u00e2ncias entre massas de probabilidade. Essa abordagem permite um tratamento mais suave de pequenas flutua\u00e7\u00f5es, resultando em uma m\u00e9trica mais est\u00e1vel que o KL Divergence ao longo do tempo</p> <p>AA dist\u00e2ncia de Wasserstein \u00e9 independente de sobreposi\u00e7\u00e3o e pode lidar muito bem com mudan\u00e7as significativas de distribui\u00e7\u00e3o em forma ou localiza\u00e7\u00e3o</p>"},{"location":"A_modeling_guideline/modeling_guideline/#psi","title":"PSI","text":""},{"location":"A_modeling_guideline/modeling_guideline/#ks_1","title":"KS","text":""},{"location":"A_modeling_guideline/modeling_guideline/#7-grupos-homogeneos","title":"7 - Grupos Homog\u00eaneos","text":"<p>Com a modelagem finalizada, ou pelo menos uma V0, \u00e9 preciso agrupar seus resultados por grupos homog\u00eaneos (GHs). A ideia de criar grupos homog\u00eaneos \u00e9 observar o desempenho do modelo para cada grupo, ao inv\u00e9s de olhar o seu desempenho geral. Pode acontecer de no geral o modelo estar bem preditivo, por\u00e9m ao quebrar em grupos observamos que determinados grupos est\u00e3o subindo a m\u00e9dia de nossa preditividade, enquanto outros est\u00e3o pouqu\u00edssimo preditivos.</p> <p>Para a cria\u00e7\u00e3o dos GHs utilizamos a fun\u00e7\u00e3o <code>grupos_risco</code> do pacote <code>inter.ai</code>, que utiliza a fun\u00e7\u00e3o <code>classInt::classIntervals</code> para encontrar o melhor ponto de corte</p> <pre><code>inter.ai::gerar_grupos_homogeneos(y_true, y_pred, try_bins = 4:20)\n</code></pre> <p>\u00c9 preciso testar diferentes combina\u00e7\u00f5es de bins para encontrar a melhor parti\u00e7\u00e3o para os grupos homog\u00eaneos, uma vez que a altera\u00e7\u00e3o dos bins m\u00e1ximos e m\u00ednimos impacta na fun\u00e7\u00e3o.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#71-matriz-de-confusao","title":"7.1 - Matriz de confus\u00e3o","text":"<p>Podemos ao inv\u00e9s de comparar os modelos, pode ser o caso de termos modelos complementares, dessa forma a compara\u00e7\u00e3o se torna um pouco mais dif\u00edcil, como \u00e9 o exemplo da cria\u00e7\u00e3o de um modelo espec\u00edfico de fim de m\u00eas e outro para meio de m\u00eas. Nesse caso, se torna interessante avaliar o qu\u00e3o acert\u00edvo ambos os modelos s\u00e3o, para isso pegamos uma mesma base de dados (por exemplo a base do meio de m\u00eas) e fazemos uma matriz de confus\u00e3o para ambos, utilizando o melhor ponto de corte para ser utilizado para</p>"},{"location":"A_modeling_guideline/modeling_guideline/#teste-de-proporcao-com-correcao-de-bonferroni","title":"Teste de Propor\u00e7\u00e3o com corre\u00e7\u00e3o de Bonferroni","text":"<p>Ap\u00f3s agrupar sua base por GH, criando uma coluna com a soma do seu target e a quantidade de observa\u00e7\u00f5es por grupo, rodamos o teste de Bonferroni.</p> <pre><code>pairwise.prop.test(x=teste$soma_target,\n                   n=teste$N_grupo,\n                   p.adjust.method = \"bonferroni\",\n                   alternative = \"two.sided\",\n                   correct = FALSE)\n</code></pre> <p>O teste de propor\u00e7\u00e3o com ajuste de Bonferroni, feito pela fun\u00e7\u00e3o acima, multiplica o p-valor encontrado pelo n\u00famero de grupos, n\u00e3o sendo necess\u00e1rio utilizar a corre\u00e7\u00e3o de \\(\\frac{\\alpha}{m}\\) manualmente para avaliar o p-valor, em que \\(H_0\\) seria os grupos serem iguais, desse modo p-valor &lt; 0.05 significa n\u00e3o ter diferen\u00e7a significativa entre as compara\u00e7\u00f5es m\u00faltiplas.</p> <p>Vale ressaltar que o teste acima faz a compara\u00e7\u00e3o m\u00faltipla de todos os grupos, podendo estar superestimando os p-valores, para o caso de GHs \u00e9 interessante colocar o <code>p.adjust.method</code> como <code>none</code> e fazer a corre\u00e7\u00e3o manualmente multiplicando o p-valor pela quantidade de compara\u00e7\u00f5es (quantidade de grupos - 1) pois dessa forma teremos para a compara\u00e7\u00e3o de A, B, C e D apenas A -&gt; B, B -&gt; C, C -&gt; D.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#8-atualizacao-do-modelo","title":"8 - Atualiza\u00e7\u00e3o do Modelo","text":"<p>A ideia \u00e9 entregar um modelo r\u00e1pido, para podermos aprender com ele e gerar valor o mais r\u00e1pido poss\u00edvel. Ap\u00f3s a primeira entrega ser feita, n\u00e3o devemos nos contentar com os resultados, pelo contr\u00e1rio, como j\u00e1 temos uma entrega devemos buscar por melhorias no modelo que antes n\u00e3o eram poss\u00edveis devido a necessidade de tempo junto a necessidade da entrega.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#9-pontos-de-atencao-aos-resultados","title":"9 - Pontos de aten\u00e7\u00e3o aos resultados","text":"<p>Entender o qu\u00e3o preditivo est\u00e1 meu modelo. Acertar muito em p\u00fablicos \u00f3bvios e pouco em p\u00fablicos mais complexos com um ks de 90 \u00e9 pior do que acertar razo\u00e1vel em p\u00fablicos mais complexos e ks de 40</p> <p>Como medir objetivamente se a resposta do meu modelo est\u00e1 de fato agregando, al\u00e9m de olhar valor do KS?</p> <p>Comparar o modelo antigo (se existir) com o modelo novo, utilizando a mesma base para efeito de compara\u00e7\u00e3o</p> <p>Mudar a vis\u00e3o de sua modelagem pode ser uma sa\u00edda vi\u00e1vel, caso esteja estagnado em sua modelagem. Uma nova abordagem, como por exemplo modelar por contrato ao inv\u00e9s de cpf, buscar features de outros lugares, tentar uma r\u00e9gra de neg\u00f3cios ao inv\u00e9s de uma modelagem mais robusta, etc, podem ser boas alternativas.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#10-pos-modelagem","title":"10 - P\u00f3s modelagem","text":"<p>Com o modelo pronto o pr\u00f3ximo passo \u00e9 o deploy, que pode ocorrer de forma autom\u00e1tica ou manual, o \"manu\u00e1tico\" consiste em um c\u00f3digo com apenas o neces\u00e1rio para conseguir rodar o modelo manualmente de acordo com a demanda. Feito isso, existem algum caminhos a se seguir, seu modelo pode subir para produ\u00e7\u00e3o ou passar por uma experimenta\u00e7\u00e3o por exemplo. No caso do experimento pode ser necess\u00e1rio um direcionamento ao cliente para que seja feito de forma correta, evitando inconsist\u00eancias futuras.</p> <p>A <code>calibra\u00e7\u00e3o de modelo</code> consiste em um conjunto de an\u00e1lises, compostos por <code>Diagn\u00f3stico</code> e <code>Remedia\u00e7\u00e3o</code> para avaliar as probabilidades retornadas pelo modelo. O objetivo da calibra\u00e7\u00e3o do modelo \u00e9 garantir que as probabilidades de classe estimadas sejam consistentes com o que ocorreria naturalmente. (No caso de estimativas pontuais PODE fazer sentido essa abordagem, por\u00e9m ao criar os GHs temos uma mitiga\u00e7\u00e3o do erro a partir da predi\u00e7\u00e3o intervalar de cada grupo com bootstrap para avaliar o intervalo de 95%)</p>"},{"location":"A_modeling_guideline/modeling_guideline/#11-modelo-nao-supervisionado","title":"11 - Modelo n\u00e3o supervisionado","text":"<p>Para modelos n\u00e3o supervisionados n\u00e3o temos um target pr\u00e9-definido para podermos utilizar de insumo para nossa modelagem, dessa forma temos que utilizar de outras t\u00e9cnicas para chegarmos em resultados satisfat\u00f3rios.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#o-que-queremos-modelar","title":"O que queremos modelar?","text":""},{"location":"A_modeling_guideline/modeling_guideline/#e-possivel-obter-uma-proxy-do-que-esperamos-ao-final-da-modelagem","title":"\u00c9 poss\u00edvel obter uma proxy do que esperamos ao final da modelagem?","text":"<p>Esse \u00e9 um t\u00f3pico sens\u00edvel pois exige bastante conhecimento acerca de nosso problema, al\u00e9m de termos que nos atentar a vi\u00e9ses para nossa modelagem, caso tenhamos alguma ideia a priori do que fazer.</p> <p>Devemos seguir com o monitoramento afim de garantirmos a funcionalidade/efic\u00e1cia do mesmo. </p>"},{"location":"A_modeling_guideline/modeling_guideline/#12-algorithms","title":"12 - Algorithms","text":""},{"location":"A_modeling_guideline/modeling_guideline/#supervised","title":"Supervised","text":""},{"location":"A_modeling_guideline/modeling_guideline/#regressao","title":"Regress\u00e3o","text":""},{"location":"A_modeling_guideline/modeling_guideline/#regressao-linear","title":"Regress\u00e3o Linear","text":""},{"location":"A_modeling_guideline/modeling_guideline/#regressao-logistica","title":"Regress\u00e3o Logistica","text":""},{"location":"A_modeling_guideline/modeling_guideline/#cubist-regression","title":"Cubist Regression","text":"<p>Modelo de regress\u00e3o que utiliza recortes simples de \u00e1rvores de decis\u00e3o e cria modelos de regress\u00e3o dentro cada regra</p> <pre><code>Cubist::cubist(\n  x = X,\n  y = Y,\n  committees = 1, \n  control = cubistControl(\n    unbiased = FALSE,\n    rules = 100,\n    extrapolation = 100,\n    sample = 0,\n    seed = sample.int(4096, size = 1) - 1L,\n    label = \"outcome\"\n    )\n  )\n</code></pre>"},{"location":"A_modeling_guideline/modeling_guideline/#binomial","title":"Binomial","text":""},{"location":"A_modeling_guideline/modeling_guideline/#binomial-negativa","title":"Binomial Negativa","text":"<p>\u00c9 mais flex\u00edvel que a Poisson, pois possui um par\u00e2metro que ajusta a vari\u00e2ncia independentemente da m\u00e9dia.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#multinomial","title":"Multinomial","text":"<p>\u00c9 mais geral que a regress\u00e3o log\u00edstica porque a vari\u00e1vel dependente n\u00e3o est\u00e1 restrita a duas categorias, mas a um n\u00famero finito maior que 2 podendo ser ordinal ou n\u00e3o.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#poisson","title":"Poisson","text":"<p>\u00c9 um caso especial da distribui\u00e7\u00e3o binomial negativa. Assume m\u00e9dia e vari\u00e2ncias iguais (tal qual a regress\u00e3o assume dados normais)</p> <p>Tem como objetivo modelar contagens de eventos infinitos (Sem restri\u00e7\u00e3o para o n\u00famero m\u00e1ximo do target). Muitas vezes funciona com vari\u00e1veis cont\u00ednuas n\u00e3o negativas</p>"},{"location":"A_modeling_guideline/modeling_guideline/#regressao-quantilica","title":"Regress\u00e3o Quant\u00edlica","text":""},{"location":"A_modeling_guideline/modeling_guideline/#regressao-zoib-zero-one-inflated-beta","title":"Regress\u00e3o zoib (Zero-One-Inflated Beta)","text":""},{"location":"A_modeling_guideline/modeling_guideline/#generalized-additive-model-gam","title":"Generalized Additive Model (GAM)","text":""},{"location":"A_modeling_guideline/modeling_guideline/#decision-tree","title":"Decision Tree","text":"<p>\u00c1rvore de decis\u00e3o baseada em regras</p>"},{"location":"A_modeling_guideline/modeling_guideline/#random-forest-ensemble","title":"Random Forest (ensemble)","text":""},{"location":"A_modeling_guideline/modeling_guideline/#gradiente-boosted-regression-trees","title":"Gradiente Boosted Regression Trees","text":""},{"location":"A_modeling_guideline/modeling_guideline/#support-vector-machine","title":"Support Vector Machine","text":"<p>Geralmente utilizado em contextos de classifica\u00e7\u00e3o, encontra um hiperplano (podendo ser uma linha) para segregar categorias</p>"},{"location":"A_modeling_guideline/modeling_guideline/#naive-bayes","title":"Naive Bayes","text":""},{"location":"A_modeling_guideline/modeling_guideline/#xgboost","title":"XGBoost","text":""},{"location":"A_modeling_guideline/modeling_guideline/#lgbm","title":"Lgbm","text":""},{"location":"A_modeling_guideline/modeling_guideline/#catboost","title":"Catboost","text":""},{"location":"A_modeling_guideline/modeling_guideline/#adaboost","title":"AdaBoost","text":"<p>Ajusta o peso das amostras em cada itera\u00e7\u00e3o, as amostras mal classificadas recebem mais peso. O algoritmo foca em corrigir as amostras dif\u00edceis. O modelo final ser\u00e1 a combina\u00e7\u00e3o de classificadores fracos ponderados.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#gradient-boosting","title":"Gradient Boosting","text":"<p>Diferente do AdaBoosting, vai ajustar os erros residuais (diferen\u00e7a entre valor real e previs\u00e3o) em cada itera\u00e7\u00e3o. Cada modelo ir\u00e1 tentar prever e corrigir os erros das rodadas anteriores.</p> <p>Exemplo: O gradient come\u00e7a com uma previs\u00e3o simples de 180 mil para todas as casas e vai ajustando a cada itera\u00e7\u00e3o as previs\u00f5es, baseando na tentativa de descobrir o erro residual. A cada rodada o modelo vai ajustar os erros residuais. Na tabela abaixo o erro inicial de +20.000 na casa 1 foi ajustado para 10.000, resultando em uma nova previs\u00e3o de 190.000. O resultado final \u00e9 a soma das corre\u00e7\u00f5es feitas em cada rodada, aproximando-se do valor real</p> Casa Pre\u00e7o Real Predi\u00e7\u00e3o (Round1) Erro Residual (Round1) Previs\u00e3o Residual Nova Predi\u00e7\u00e3o 1 200.000 180.000 +20.000 +10.000 190.000 2 300.000 180.000 +120.000 +70.000 250.000 3 250.000 180.000 +70.000 +40.000 220.000"},{"location":"A_modeling_guideline/modeling_guideline/#rusboost","title":"RusBoost","text":""},{"location":"A_modeling_guideline/modeling_guideline/#bagging-bootstrap-aggregation","title":"Bagging (Bootstrap Aggregation)","text":"<p>Em caso de termos muitas vari\u00e1veis categ\u00f3ricas o Catboost \u00e9 utilizado, evitando ter que fazer diversas dummies para a modelagem</p>"},{"location":"A_modeling_guideline/modeling_guideline/#neural-network","title":"Neural Network","text":"<p>Se remover as camadas ocultas de uma rede neural ela se torna apenas um classificador linear simples, ou seja, pode lidar com tarefas diretas e linearmente separ\u00e1veis como AND e OR mas n\u00e3o pode lidar com XOR (OR exclusivo) que precisa de pelo menos uma camada oculta (junto da fun\u00e7\u00e3o de ativa\u00e7\u00e3o linear) para capturar seu padr\u00e3o n\u00e3o linear</p>"},{"location":"A_modeling_guideline/modeling_guideline/#recurrent-neural-network","title":"Recurrent Neural Network","text":""},{"location":"A_modeling_guideline/modeling_guideline/#convolutional-neural-network","title":"Convolutional Neural Network","text":""},{"location":"A_modeling_guideline/modeling_guideline/#graph-neural-network","title":"Graph Neural Network","text":""},{"location":"A_modeling_guideline/modeling_guideline/#unsupervised","title":"Unsupervised","text":""},{"location":"A_modeling_guideline/modeling_guideline/#hierarchical-clustering","title":"Hierarchical Clustering","text":""},{"location":"A_modeling_guideline/modeling_guideline/#k-clustering","title":"K- Clustering","text":""},{"location":"A_modeling_guideline/modeling_guideline/#k-means","title":"K-Means","text":""},{"location":"A_modeling_guideline/modeling_guideline/#k-median","title":"K-Median","text":""},{"location":"A_modeling_guideline/modeling_guideline/#k-modes","title":"K-Modes","text":""},{"location":"A_modeling_guideline/modeling_guideline/#mini-batch-k-means-clustering","title":"Mini Batch K-Means Clustering","text":""},{"location":"A_modeling_guideline/modeling_guideline/#fuzzy-k-modes","title":"Fuzzy K-Modes","text":"<p>\u00c9 uma extens\u00e3o do K-Modes, que ao inv\u00e9s de apontar cada objeto a um cluster, calcula o valor do grau de proximidade para cada objeto em cada cluster</p>"},{"location":"A_modeling_guideline/modeling_guideline/#fuzzy-c-means","title":"Fuzzy C-Means","text":"<p>\u00c9 uma vers\u00e3o probabilistica do K-Means. Associa todos os objetos em todos os clusters, sendo que a soma de todas as associa\u00e7\u00f5es \u00e9 1. Desse modo todos os clusters tem uma associa\u00e7\u00e3o cont\u00ednua (diferentemente do K-Means que \u00e9 discreta) com cada cluster em rela\u00e7\u00e3o a cada outro cluster</p> <p>O algoritmo atribui iterativamente e computa o centr\u00f3ide dos clusters igual ao K-Means at\u00e9 que qualquer fun\u00e7\u00e3o de crit\u00e9rio seja otimizada ou a converg\u00eancia caia abaixo de um threshold pr\u00e9-determinado.ld value.</p> <p>Esse algoritmo n\u00e3o \u00e9 rigoroso como o K-Means na atribui\u00e7\u00e3o e funciona bem para conjuntos de dados sobrepostos. No entanto, tem a mesma desvantagem que o K-Means de ter uma suposi\u00e7\u00e3o pr\u00e9via do n\u00famero de clusters. Al\u00e9m disso, um valor de threshold baixo fornece melhores resultados, mas \u00e9 mais caro do ponto de vista computacional.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#dbscan-clustering","title":"DBSCAN Clustering","text":""},{"location":"A_modeling_guideline/modeling_guideline/#gaussian-mixture-model-gmm","title":"Gaussian Mixture Model (GMM)","text":""},{"location":"A_modeling_guideline/modeling_guideline/#jenks","title":"Jenks","text":"<p>Jenks, Fisher e Fisher-Jenks</p>"},{"location":"A_modeling_guideline/modeling_guideline/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>T\u00e9cnica de redu\u00e7\u00e3o de dimensionalidade</p>"},{"location":"A_modeling_guideline/modeling_guideline/#otimizacoes","title":"Otimiza\u00e7\u00f5es","text":"<p>Antes de definir qual abordagem utilizar faz-se necess\u00e1rio responder algumas perguntas:</p> <ul> <li>Seu problema tem uma fun\u00e7\u00e3o objetiva definida?</li> <li>Seu problema tem solu\u00e7\u00e3o linear?</li> <li>Seu problema tem um espa\u00e7o amostral convexo? (seu \u00f3timo local tamb\u00e9m \u00e9 global, ent\u00e3o encontrar uma solu\u00e7\u00e3o local \u00e9 suficiente)</li> </ul> <p>Respondidas as perguntas podemos optar por algoritmos de busca mais simples ou mais complexos, mas para essa decis\u00e3o pode ser interessante levar em considera\u00e7\u00e3o o Teorema de Bolzano-Weierstrass:</p> <p>A complexidade de uma problema de otimiza\u00e7\u00e3o est\u00e1 diretamente relacionado ao tamanho do Espa\u00e7o de Busca coorespondente. O processo de solu\u00e7\u00e3o de um problema pode ser reduzido a um Algoritmo de Busca Heur\u00edstica, cujo Espa\u00e7o de Busca \u00e9 formado por transforma\u00e7\u00f5es sucessivas de Estados em uma certa ordem de gera\u00e7\u00e3o e percurso, enquanto Algoritmos mais simples podem ir diretamente ao ponto \u00f3timo.</p> <p>Algoritmos heur\u00edsticos n\u00e3o garantem encontrar a solu\u00e7\u00e3o \u00f3tima, mas podem encontrar boas solu\u00e7\u00f5es em tempo menor (no caso de problemas mais complexos) e s\u00e3o extremamente \u00fateis quando o espa\u00e7o de busca \u00e9 grande ou a estrutura n\u00e3o \u00e9 bem definida.</p> <p>Podemos utilizar algoritmos heur\u00edsitocs quando:   - o espa\u00e7o de busca \u00e9 grande, tornando a busca simples impratic\u00e1vel - uma solu\u00e7\u00e3o aproximada ou sub\u00f3tima \u00e9 aceit\u00e1vel - os dados s\u00e3o complexos e din\u00e2micos</p>"},{"location":"A_modeling_guideline/modeling_guideline/#algoritmos-convexos","title":"Algoritmos Convexos","text":""},{"location":"A_modeling_guideline/modeling_guideline/#simplex","title":"Simplex","text":"<p>Se o seu problema tiver uma solu\u00e7\u00e3o linear, voc\u00ea pode usar o M\u00e9todo Simplex. Ele come\u00e7a na borda do poliedro das solu\u00e7\u00f5es vi\u00e1veis porque os v\u00e9rtices s\u00e3o candidatos a solu\u00e7\u00f5es \u00f3timas e permite uma explora\u00e7\u00e3o eficiente do espa\u00e7o de solu\u00e7\u00f5es usando a geometria dos problemas lineares. Isso se deve ao fato de que a solu\u00e7\u00e3o \u00f3tima de uma fun\u00e7\u00e3o linear, se existir, est\u00e1 em um v\u00e9rtice, j\u00e1 que fun\u00e7\u00f5es lineares atingem m\u00e1ximos ou m\u00ednimos nos extremos, uma vez que n\u00e3o tem curvatura para alcan\u00e7ar \u00f3timos no interior. Desta forma, o Simplex pode explorar os v\u00e9rtices sistematicamente sem computar todo o espa\u00e7o de solu\u00e7\u00f5es.</p>"},{"location":"A_modeling_guideline/modeling_guideline/#ordinary-least-squares-ols","title":"Ordinary Least Squares (OLS)","text":""},{"location":"A_modeling_guideline/modeling_guideline/#algoritmos-heuristicos","title":"Algoritmos Heur\u00edsticos","text":""},{"location":"A_modeling_guideline/modeling_guideline/#buscas-em-profundidade-limitada","title":"Buscas em profundidade limitada","text":""},{"location":"A_modeling_guideline/modeling_guideline/#simulated-annealing","title":"Simulated Annealing","text":""},{"location":"A_modeling_guideline/modeling_guideline/#genetic-algorithms","title":"Genetic Algorithms","text":""},{"location":"A_modeling_guideline/modeling_guideline/#12-model-monitoring","title":"12 - Model Monitoring","text":"<p>Quando temos um modelo em produ\u00e7\u00e3o, seja online ou em batch devemos fazer seu monitoramento afim de garantirmos a funcionalidade/efic\u00e1cia do mesmo. Podemos utilizar como material de consulta o <code>Working Paper No. 14</code></p>"},{"location":"A_modeling_guideline/modeling_guideline/#121","title":"12.1","text":"<p>O que?   - Consist\u00eancia no resultado do modelo Como?   - Taxa de aprova\u00e7\u00f5es est\u00e1vel, conversando com a taxa esperada do modelo</p>"},{"location":"A_modeling_guideline/modeling_guideline/#122","title":"12.2","text":"<p>O que?   - Comportamento dos grupos se manteve?   Como?   - Taxa de aprova\u00e7\u00f5es x taxa de bads dos GHs</p>"},{"location":"A_modeling_guideline/modeling_guideline/#123","title":"12.3","text":"<p>O que?   - Continuamos tratando da mesma popula\u00e7\u00e3o de quando treinamos?   Como?   - Observar o drift das features e seu descolocamento em fun\u00e7\u00e3o da m\u00e9dia</p>"},{"location":"A_modeling_guideline/modeling_guideline/#124","title":"12.4","text":"<p>O que?   - Avalia\u00e7\u00e3o da qualidade do ajuste do modelo. (O modelo ainda pode explicar os dados observados?) Como?   - Teste de Hosmer e Lemeshow para avaliar se as propor\u00e7\u00f5es observadas e esperadas s\u00e3o as mesmas ao longo da amostra</p>"},{"location":"A_modeling_guideline/modeling_guideline/#13-append","title":"13 - Append","text":""},{"location":"A_modeling_guideline/modeling_guideline/#mathematical-definitions-in-data-science","title":"Mathematical definitions in data science","text":""},{"location":"A_modeling_guideline/modeling_guideline/#gradient-descent","title":"Gradient Descent","text":"<p>\\(\\(\\theta_{j+1} = \\theta_{j} - \\alpha \\nabla J(\\theta_{j})\\)\\) ### Normal Distribution</p> \\[f(x|\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp(- \\frac{(x-\\mu)^2}{2\\sigma^2})\\]"},{"location":"A_modeling_guideline/modeling_guideline/#z-score","title":"Z-score","text":"\\[z = \\frac{x-\\mu}{\\sigma}\\]"},{"location":"A_modeling_guideline/modeling_guideline/#sigmoid","title":"Sigmoid","text":"\\[\\theta(x) = \\frac{1}{1+e^{-x}}\\]"},{"location":"A_modeling_guideline/modeling_guideline/#correlation","title":"Correlation","text":"\\[Correlation = \\frac{Cov(X,Y)}{Std(X) \\dot{} Std(Y)}\\]"},{"location":"A_modeling_guideline/modeling_guideline/#cosine-similarity","title":"Cosine Similarity","text":"\\[Similarity = \\frac{A \\dot{} B}{||A|| \\dot{} ||B||}\\] <p>Avi Chawla Save posts orrelation = \\frac{Cov(X,Y)}{Std(X) \\dot{} Std(Y)}$$</p>"},{"location":"A_modeling_guideline/modeling_guideline/#cosine-similarity_1","title":"Cosine Similarity","text":"\\[Similarity = \\frac{A \\dot{} B}{||A|| \\dot{} ||B||}\\] <p>Avi Chawla Save posts</p>"},{"location":"B_artifacts/discovery/","title":"\ud83e\uddfe Discovery","text":"<p>teste teste</p>"},{"location":"B_artifacts/model_approval/","title":"\ud83c\udd97 Model Approval","text":"<p>b</p>"},{"location":"B_artifacts/technical_documentation/","title":"\ud83d\udcdd Technical Documentation","text":"<p>c</p>"},{"location":"C_patterns/data_preparation/","title":"Data Preparation","text":""},{"location":"C_patterns/data_preparation/#data-preparation","title":"Data Preparation","text":""},{"location":"C_patterns/data_preparation/#feature-prefix","title":"Feature prefix:","text":"prefix description vl_ value nm_ name dc_ description fl_ flag"},{"location":"C_patterns/data_preparation/#feature-time-suffix","title":"Feature time suffix:","text":"<p>You can understand t as a month, a day, a week, a period, like a quarter, etc.</p> suffix description t0 time of reference t1 time 1 unit after the reference t0_t1 the aggregate time between the time of reference and the next unit of time t1_t0 the aggregate time between the time of reference and the previous unit of time"},{"location":"D_statistical_concepts/accuracy/","title":"Accuracy","text":"\\[\\frac{TP+TN}{TP+TN+FP+FN}\\]"},{"location":"D_statistical_concepts/bagging/","title":"Bagging","text":""},{"location":"D_statistical_concepts/bagging/#bagging-bootstrap-aggregation","title":"Bagging (Bootstrap Aggregation)","text":""},{"location":"D_statistical_concepts/boosting/","title":"Boosting","text":"<p>Boosting \u00e9 a fam\u00edlia de m\u00e9todos baseados em aprendizado sequencial de modelos fracos (normalmente, \u00e1rvores rasas)</p> <p>Cada novo modelo tenta corrigir os erros do anterior</p> <p>No final, os modelos s\u00e3o combinados (por m\u00e9dia ponderada ou soma) para formar um preditor mais forte</p> <p>\u27a1\ufe0f Ideia central: \"Vamos aprender com os erros anteriores para melhorar passo a passo.\"</p>"},{"location":"D_statistical_concepts/boosting/#adaboost","title":"AdaBoost","text":""},{"location":"D_statistical_concepts/boosting/#gradient-boosting","title":"Gradient Boosting","text":"<p>Gradient Boosting \u00e9 uma forma espec\u00edfica de Boosting que:</p> <p>Baseia o processo de corre\u00e7\u00e3o de erros em gradientes do erro (fun\u00e7\u00e3o de perda)</p> <p>Em cada etapa, o novo modelo \u00e9 treinado para prever o gradiente negativo do erro (ou seja, a dire\u00e7\u00e3o de maior melhora na fun\u00e7\u00e3o de perda)</p> <p>Isso torna o treinamento mais matematicamente fundamentado e flex\u00edvel, pois funciona com qualquer fun\u00e7\u00e3o de perda diferenci\u00e1vel (ex: MSE, log-loss)</p>"},{"location":"D_statistical_concepts/boosting/#gradient-boosting-regressor","title":"Gradient Boosting Regressor","text":"<p>O GradientBoostingRegressor implementa o algoritmo cl\u00e1ssico de Gradient Boosting:</p> <p>Ele treina v\u00e1rias \u00e1rvores de decis\u00e3o sequencialmente, onde cada \u00e1rvore aprende a corrigir os erros cometidos pelas anteriores.</p> <p>Cada \u00e1rvore \u00e9 constru\u00edda usando o conjunto completo de amostras e features cont\u00ednuas, realizando splits com base em ganhos de informa\u00e7\u00e3o para valores exatos.</p> <p>\ud83d\udcc9 Esse processo \u00e9 preciso, mas computacionalmente caro quando temos muitos dados e valores cont\u00ednuos, porque ele precisa testar todos os poss\u00edveis pontos de divis\u00e3o.</p> <p>Usamos quando: - Seus dados s\u00e3o pequenos/m\u00e9dios - Voc\u00ea precisa de precis\u00e3o nos splits - Quer explica\u00e7\u00f5es mais detalhadas dos crit\u00e9rios de divis\u00e3o</p> <p>Em GradientBoostingRegressor e outros modelos como DecisionTreeRegressor, um valor NaN \u00e9 um problema, pois o algoritmo precisa comparar valores num\u00e9ricos para fazer os splits, mas NaN n\u00e3o pode ser comparado com operadores como &lt;, &gt;, ==, etc. Desse modo o modelo quebra com erro ou exige que voc\u00ea impute os valores antes (m\u00e9dia, mediana, etc). Nesse caso podemos utilizar o HistGradientBoostingRegressor</p>"},{"location":"D_statistical_concepts/boosting/#histogram-gradient-boosting-regressor","title":"Histogram Gradient Boosting Regressor","text":"<p>O HistGradientBoostingRegressor \u00e9 uma vers\u00e3o otimizada inspirada no LightGBM, introduzida no scikit-learn a partir da vers\u00e3o 0.22.</p> <p>A grande diferen\u00e7a:</p> <p>Ele constr\u00f3i histogramas para representar intervalos de valores cont\u00ednuos \u2014 ao inv\u00e9s de testar todos os valores poss\u00edveis para splits, ele s\u00f3 testa os limites dos bins.</p> <p>Isso traz v\u00e1rias vantagens: \u2705 Muito mais r\u00e1pido e eficiente com grandes bases.</p> <p>\u2705 Suporte nativo a valores faltantes (NaN) \u2014 eles s\u00e3o tratados como uma categoria separada.</p> <p>\u2705 Mem\u00f3ria muito mais eficiente.</p> <p>\u26a0\ufe0f Pequena perda de precis\u00e3o nos splits (porque usa aproxima\u00e7\u00f5es via histogramas).</p> <p>Usamos quando:</p> <ul> <li>Voc\u00ea tem muitos dados ou features cont\u00ednuas</li> <li>Precisa de performance</li> <li>Tem valores ausentes (NaN)</li> <li>Est\u00e1 acostumado com LightGBM ou CatBoost</li> </ul> <p>Quando o HistGradientBoostingRegressor encontra uma feature com valores faltantes, ele:</p> <ol> <li>Cria um grupo separado para os NaNs (eles s\u00e3o um bin separado).</li> <li>Durante o treinamento, ele aprende a onde enviar os NaN no split:<ul> <li>Para o lado direito da \u00e1rvore?</li> <li>Para o lado esquerdo?</li> <li>Ele testa ambas as op\u00e7\u00f5es e escolhe a que resulta em menor erro.</li> </ul> </li> </ol>"},{"location":"D_statistical_concepts/boosting/#rusboost","title":"RusBoost","text":""},{"location":"D_statistical_concepts/calibration/","title":"Calibration","text":"<p>github.com/numeristical/resources/blob/master/CalibrationWorkshop/</p>"},{"location":"D_statistical_concepts/central_limit_theorem/","title":"Central Limit Theorem","text":"<p>The Central Limit THeorem states that the sampling distribution of the sample mean becomes approximately normal, regardless of the population's original distribution, as the sample size becomes large enough.</p> <ul> <li>Works even if the original data is not normally distributed</li> <li>Sample size should be sufficiently large (n&gt;30)</li> <li>The mean of the sample means $ \\approx$ population mean</li> </ul>"},{"location":"D_statistical_concepts/chisq_test/","title":"Chi-Squared","text":"<p>Chi-Squared Test for Goodness of Fit</p>"},{"location":"D_statistical_concepts/correlation/","title":"Correlation","text":"<p>Measures how strongly two variable move together</p> <ul> <li>Positive Correlation: Both variables move in the same disrection</li> <li>Negative Correlation: Variable move in opposite</li> <li>No Correlation: No clear relationship between variables</li> </ul>"},{"location":"D_statistical_concepts/correlation/#correlation-matriz","title":"Correlation matriz?","text":""},{"location":"D_statistical_concepts/correlation/#kendall","title":"Kendall","text":"<p>Is a non-parametric test that measures the strength of dependence between two variables</p> \\[ \\tau = a \\]"},{"location":"D_statistical_concepts/correlation/#pearson","title":"Pearson","text":"<p>Both variable should be normally distributed and need assumptions like linearity (assumes a straight line relationship between each of the two variables) and homoscedasticity (data is equally distributed about the regression line)</p>"},{"location":"D_statistical_concepts/correlation/#spearman","title":"Spearman","text":"<p>Spearman rank correlation is a non-parametric test that measures the degree of association between two variables. </p>"},{"location":"D_statistical_concepts/correlation/#point-biserial-correlation","title":"Point-Biserial Correlation","text":""},{"location":"D_statistical_concepts/cross_validation/","title":"Cross-Validation","text":""},{"location":"D_statistical_concepts/cross_validation/#traintest-split","title":"Train/Test Split","text":"<p>Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.</p>"},{"location":"D_statistical_concepts/cross_validation/#loocv","title":"LOOCV","text":"<p>Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.</p>"},{"location":"D_statistical_concepts/cross_validation/#stratified","title":"Stratified","text":"<p>The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.</p>"},{"location":"D_statistical_concepts/cross_validation/#repeated","title":"Repeated","text":"<p>This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample.</p>"},{"location":"D_statistical_concepts/cross_validation/#nested","title":"Nested","text":"<p>This is where k-fold cross-validation is performed within each fold of cross-validation, often to perform hyperparameter tuning during model evaluation. This is called nested cross-validation or double cross-validation.</p> <p>References:</p> <p>Sklearn Cross Validation</p> <p>https://machinelearningmastery.com/k-fold-cross-validation/</p>"},{"location":"D_statistical_concepts/homoscedasticity/","title":"Homoscedasticity","text":"<p>a</p>"},{"location":"D_statistical_concepts/kl_divergence/","title":"Kullback-Leibler Divergence","text":"<p>Kullback-Leibler Divergence</p>"},{"location":"D_statistical_concepts/ks/","title":"Kolmogorov Smirnov","text":""},{"location":"D_statistical_concepts/kurtosis/","title":"Kurtosis","text":"<p>Kurtosis measures the 'tailedness' or heaviness of the tails of the distribution compared to a normal distribution</p>"},{"location":"D_statistical_concepts/kurtosis/#leptokurtic-high-kurtosis","title":"Leptokurtic (High Kurtosis)","text":"<p>Heavy tails, sharp peak Ex: A distribuition with a lot of outliers</p>"},{"location":"D_statistical_concepts/kurtosis/#platykortic-low-kurtosis","title":"Platykortic (Low Kurtosis)","text":"<p>Light tails, flatter peak Ex: A flat, spread-out distribution</p>"},{"location":"D_statistical_concepts/kurtosis/#mesokurt-normal-kurtosis","title":"Mesokurt (Normal Kurtosis)","text":"<p>Similar to normal distribution</p>"},{"location":"D_statistical_concepts/mae/","title":"Mean Absolute Error","text":"<p>O MAE (Mean Absolute Error) \u2014 ou Erro Absoluto M\u00e9dio \u2014 \u00e9 uma m\u00e9trica de avalia\u00e7\u00e3o que mede, em m\u00e9dia, o quanto as previs\u00f5es do modelo est\u00e3o distantes dos valores reais, sem considerar a dire\u00e7\u00e3o do erro (positivo ou negativo).</p> \\[MAE = \\frac{1}{n}\\sum{n}{i=1}|y_i - \\hat{y_i}|\\] <p>\\(y_i\\): valor real \\(\\hat{y_i}\\): valor previsto \\(n\\): n\u00famero de observa\u00e7\u00f5es</p>"},{"location":"D_statistical_concepts/multicollinearity/","title":"Multicollinearity","text":"<p>When two or more independent variables in a statistical model are strongly correlated.</p> <p>How detect?</p> <ul> <li>Correlation Matriz</li> <li>VIF</li> <li>Principal Components Analysis</li> </ul>"},{"location":"D_statistical_concepts/outliers/","title":"Outliers","text":"<p>Outliers are data points significantly different from most other data points in a dataset.</p> <p>We can detect outliers using some methods:</p> <p>IQR Method:</p> <ul> <li>Lower Bound = Q1 - 1.5 * IQR</li> <li>Upper Bound = Q3 + 1.5 * IQR Any value outide this range is an outlier</li> </ul> <p>Standard Deviation (for normal distribution):</p> <p>Data points beyond +-2sigma or +- 3sigma from the mean are often considered outliers</p>"},{"location":"D_statistical_concepts/precision/","title":"Precison","text":"\\[\\frac{TP}{TP+FP}\\]"},{"location":"D_statistical_concepts/precision_recall_curve/","title":"Precision-Recall Curve","text":"<p>Area under curve of Precision vs Recall</p>"},{"location":"D_statistical_concepts/recall/","title":"Recall","text":"\\[\\frac{TP}{TP+FN}\\]"},{"location":"D_statistical_concepts/regularization/","title":"Regularization","text":""},{"location":"D_statistical_concepts/regularization/#l1","title":"L1","text":""},{"location":"D_statistical_concepts/regularization/#l2","title":"L2","text":""},{"location":"D_statistical_concepts/roc_auc/","title":"ROC AUC","text":"<p>Area under curve of TPR and FPR</p>"},{"location":"D_statistical_concepts/sampling/","title":"Sampling","text":"<p>Sampling is the process of selecting a subset of individuals from a larger population to estimate characteristics of the whole group.</p>"},{"location":"D_statistical_concepts/sampling/#random-sampling","title":"Random Sampling","text":""},{"location":"D_statistical_concepts/sampling/#stratified-sampling","title":"Stratified Sampling","text":""},{"location":"D_statistical_concepts/sampling/#with-and-without-reposition","title":"With and without reposition","text":""},{"location":"D_statistical_concepts/seasonality/","title":"Seasonality","text":"<p>a</p>"},{"location":"D_statistical_concepts/skewness/","title":"Skewness","text":"<p>Measures the asymmetry of the data distribution</p>"},{"location":"D_statistical_concepts/skewness/#positive-skew-right-skew","title":"Positive Skew (Right Skew)","text":"<p>Tail longet on the right</p>"},{"location":"D_statistical_concepts/skewness/#negative-skew-left-skew","title":"Negative Skew (Left Skew)","text":"<p>Tail longer on the left</p>"},{"location":"D_statistical_concepts/skewness/#zero-skew","title":"Zero Skew","text":"<p>Symmetrical like a perfect normal distribution</p>"},{"location":"D_statistical_concepts/specificity/","title":"Specificity","text":"\\[\\frac{TN}{TN+FP}\\]"},{"location":"D_statistical_concepts/standard_deviation/","title":"Standard Deviation","text":"<p>Measures how spread out the data points are, in the same units as the data.</p> <p>It's calculate as the square root of variance</p>"},{"location":"D_statistical_concepts/trend/","title":"Trend","text":"<p>a</p>"},{"location":"D_statistical_concepts/variance/","title":"Variance","text":"<p>Measures how far each data point is from the mean</p> \\[ \\sigma^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n - 1}\\] <p>\\(x_i\\) = the value of each observation</p> <p>\\(\\bar{x}\\) = the mean of all observations</p> <p>\\(n\\) =  the number of observarions</p>"},{"location":"D_statistical_concepts/volatility/","title":"Volatility","text":"<p>a</p>"},{"location":"E_glossary/bank_context/approval_rating/","title":"Approval Rating","text":"<p>Calculated by the relation between the total of clients approved / total of clients analized by a credit analisys</p>"},{"location":"E_glossary/bank_context/bnpl/","title":"BNPL","text":"<p>a</p>"},{"location":"E_glossary/bank_context/cac/","title":"CAC (Customer Acquisition Cost)","text":"<p>O CAC, ou tamb\u00e9m conhecido como Custo de Aquisi\u00e7\u00e3o do CLiente, equivale ao custo total de vendas e marketing dividido pelo n\u00famero de neg\u00f3cios fechados. Primeiramente vamos ver como \u00e9 feito esse c\u00e1lculo para facilitar o entendimento</p> \\[CAC = \\frac{Custo para aquisi\u00e7\u00e3o do cliente}{N\u00famero de novos clientes}\\] <p>Desse modo podemos observar que o CAC \u00e9 o custo m\u00e9dio gasto pela institui\u00e7\u00e3o para a aquisi\u00e7\u00e3o de novos clientes. Esse indicador deve ser analisado dentro de um intervalo estabelecido, seja mensal, trimestral, anual ou como for de interesse da institui\u00e7\u00e3o. Dentre os custos para aquisi\u00e7\u00e3o do cliente est\u00e3o os gastos com</p>"},{"location":"E_glossary/bank_context/cac/#marketing","title":"Marketing","text":"<ul> <li>Compra de an\u00fancios</li> <li>Participa\u00e7\u00e3o em eventos</li> <li>Assessoria de imprensa</li> <li>Materiais impressos</li> <li>Etc...</li> </ul>"},{"location":"E_glossary/bank_context/cac/#operacional","title":"Operacional","text":"<p>Todos os custos de estrutura:</p> <ul> <li>Sal\u00e1rios</li> <li>Comiss\u00e3o de vendas</li> <li>Treinamentos</li> <li>Aquisi\u00e7\u00e3o de ferramentas</li> <li>Assinatura de softwares</li> <li>Etc...</li> </ul>"},{"location":"E_glossary/bank_context/cac/#como-mensurar-o-cac","title":"Como mensurar o CAC?","text":"<p>Com base nos nossos gastos e no n\u00famero de novos clientes podemos calcular nosso CAC, mas como saber se esse valor est\u00e1 alto ou baixo? Podemos mensurar de duas maneiras:</p> <ol> <li>Observando o gasto m\u00ednimo necess\u00e1rio para que a capta\u00e7\u00e3o do cliente seja feita, levando em considera\u00e7\u00e3o nossos gastos operacionais e de marketing</li> <li>Comparando o valor do nosso LTV com o CAC para saber se o retorno do cliente est\u00e1 compensando o gasto que temos para capt\u00e1-lo (LTV deve ser maior que o CAC)</li> </ol>"},{"location":"E_glossary/bank_context/cac/#detalhes-cac","title":"Detalhes CAC","text":"<p>Os indicadores que entram no CAC dependem da institui\u00e7\u00e3o, mas podem ser eles:</p> Indicadores Crivo Neoway Embossing + Manuseio Postagem Anti Fraude Equipe Abertura de Conta Serpro/Serpro(Denatran)/Serpro(Datavalid) EmailAge OCR (Google Cloud) Facial Rekognition (AWS) AeC (equipe de abertura PF) Simply(OCR) DETECFLEX(Softon) Bacen(SCR) Tempest Acesso Digital Inovamind(OCR) Custo de Marketing Digital <p>Algumas \u00e1reas podem n\u00e3o entrar nos custos do CAC, por\u00e9m entram na conta do CTS</p>"},{"location":"E_glossary/bank_context/churn/","title":"Churn","text":"<p>Churn, ou \u00cdndice de Cancelamento de Clientes, \u00e9 a m\u00e9trica que indica quanto a institui\u00e7\u00e3o perdeu de cliente. O churn rate, ou taxa de cancelamento de cliente, se torna um balizador fundamental para analisarmos eventuais oportunidades de neg\u00f3cio que estamos perdendo ou at\u00e9 mesmo servi\u00e7os que n\u00e3o estamos prestando com excel\u00eancia.</p> \\[ Churn = \\frac{n\u00famero de contas encerradas}{n\u00famero de contas abertas}\\] <p>Para sabermos nossa reten\u00e7\u00e3o de clientes basta analisarmos o complementar do churn</p> \\[ Reten\u00e7\u00e3o = 1 - churn\\]"},{"location":"E_glossary/bank_context/cogs/","title":"COGS (Cost of Goods Sold)","text":"<p>\u00c9 o c\u00e1lculo de todos os custos incorridos para que houvesse a produ\u00e7\u00e3o ou venda de determinado produto. Essa m\u00e9trica n\u00e3o nos ajuda a entender se um produto ou a\u00e7\u00e3o est\u00e1 nos gerando lucro ou preju\u00edzo pois seu c\u00e1lculo n\u00e3o leva em considera\u00e7\u00e3o custos indiretos, como despesas gerais e marketing, temos apenas os gastos diretos como: m\u00e3o de obra, materiais e despesas gerais da fabrica\u00e7\u00e3o.</p> <p>Com a diferen\u00e7a entre o COGS e a receita da empresa temos o lucro bruto. Por se tratar de um custo de fazer neg\u00f3cios ele \u00e9 registrado como despesa comercial nas demonstra\u00e7\u00f5es dos resultados.</p>"},{"location":"E_glossary/bank_context/credit_cycle/","title":"Credit Cycle","text":"<p>Ciclo de cr\u00e9dito</p>"},{"location":"E_glossary/bank_context/csi/","title":"CSI","text":"<p>b</p>"},{"location":"E_glossary/bank_context/cts/","title":"CTS","text":"<p>CTS (Cost to Serve)</p> <p>O Cost to Serve \u00e9 uma ferramenta para gerir a rentabilidade dos clientes, se trata da consolida\u00e7\u00e3o de todos os custos necess\u00e1rios para fornecer o produto ao cliente, considerando o ciclo do pedido, incluindo todos os fatores de custo, desde o atendimento do cliente at\u00e9 a produ\u00e7\u00e3o de um produto. Essa an\u00e1lise de custo permite identificar \u00e1reas n\u00e3o lucrativas para mitigar perdas maiores em caso de maior volumetria das opera\u00e7\u00f5es.</p> <p>De maneira geral, a partir da an\u00e1lise de custos podemos observar quais produtos est\u00e3o no verde e quais n\u00e3o est\u00e3o, para psoteriormente avaliarmos se faz sentido manter ou n\u00e3o determinado produto ou servi\u00e7o. </p>"},{"location":"E_glossary/bank_context/cts/#calculo","title":"C\u00e1lculo","text":"<p>Com base no Relat\u00f3rio Gerencial temos como \u00e9 efetuado o c\u00e1lculo do CTS e suas vari\u00e1veis:</p> \\[ \\frac{[Despesas - cashback - (CAC \\times Novos Clientes)] \\times 4}{N\u00famero de contas digitais}\\] <p>No c\u00e1lculo temos englobado dentro de desepesas: administrativa, de pessoal e operacional. Por o cashback se tratar de um benef\u00edcio dado ao cliente, e n\u00e3o um custo necess\u00e1rio para fornecer o produto, ele precisa ser removido da conta. Multiplicamos o CAC pelo n\u00famero de novos clientes para termos o gasto total de aquisi\u00e7\u00e3o de clientes naquele per\u00edodo. Esse resultado \u00e9 multiplicado por 4 para termos uma estimativa anualizada do CTS, o total \u00e9 dividido pelo n\u00famero total de contas para chegarmos ao CTS por cliente.</p>"},{"location":"E_glossary/bank_context/cts/#o-que-compoe-o-cts","title":"O que comp\u00f5e o CTS","text":"<ul> <li>An\u00e1lise de cr\u00e9dito</li> <li>Despesas administrativas</li> <li>Impress\u00e3o e envio do cart\u00e3o</li> <li>Manuten\u00e7\u00e3o do cart\u00e3o</li> <li>Manuten\u00e7\u00e3o dos benef\u00edcios</li> <li>Cashback</li> </ul>"},{"location":"E_glossary/bank_context/ead/","title":"EAD","text":"<p>EAD (Exposure at Default)</p> <p>A exposi\u00e7\u00e3o \u00e0 inadimpl\u00eancia \u00e9 o valor previsto de perda que uma institui\u00e7\u00e3o pode estar exposta quando um devedor deixa de pagar um empr\u00e9stimo.</p> <p>O c\u00e1lculo do EAD \u00e9 feito para cada empr\u00e9stimo, de modo que esse montante \u00e9 o risco total de inadimpl\u00eancia, vale ressaltar que a medida que os empr\u00e9stimos v\u00e3o sendo pagos o EAD se altera, sendo um indicador din\u00e2mico.</p> <p>Em um contexto de cr\u00e9dito, considerar o EAD como sendo 100% do valor em atraso \u00e9 ser extremamente conservador.</p>"},{"location":"E_glossary/bank_context/ecl/","title":"ECL","text":"<p>ECL</p>"},{"location":"E_glossary/bank_context/fgc/","title":"FGC","text":"<p>O FGC \u00e9 uma institui\u00e7\u00e3o privada, sem fins lucrativos, que tem como objetivo garantir aos clientes das institui\u00e7\u00f5es financeiras associadas a recupera\u00e7\u00e3o de todo ou parte do patrim\u00f4mio investido, em caso de decreta\u00e7\u00e3o de regime de interven\u00e7\u00e3o ou liquida\u00e7\u00e3o extrajudicial. Vale lembrar que esse fundo n\u00e3o protege todas as modalidades de investimentos, dentre as que ele protege temos:</p> <ul> <li>Dep\u00f3ito \u00e0 vista ou sac\u00e1veis mediante aviso pr\u00e9vio</li> <li>Dep\u00f3sitos d epoupan\u00e7a</li> <li>CDB (Certificadod e Dep\u00f3sito Banc\u00e1rio)</li> <li>RDB (Recebidos de Dep\u00f3sitos Banc\u00e1rios)</li> <li>LCI e LCA (Letras de Cr\u00e9dito Imobili\u00e1rio e do Agroneg\u00f3cio)</li> <li>LC (Letras de C\u00e2mbio)</li> <li>LH (Letras Hipotec\u00e1rias)</li> </ul> <p>O teto do FGC \u00e9 de R\\(250 mil por institui\u00e7\u00e3o financeira, limitado a R\\)1 milh\u00e3o por CPF, somando diferentes institui\u00e7\u00f5es</p>"},{"location":"E_glossary/bank_context/float/","title":"Float","text":"<p>Por meio, por exemplo, de CDB (Certificado de Dep\u00f3sito Banc\u00e1rio) a institui\u00e7\u00e3o pega esse dinheiro gasto no cart\u00e3o de cr\u00e9dito ou parado em conta corrente e o movimenta afim de gerar receita</p>"},{"location":"E_glossary/bank_context/fpd/","title":"FPD (FIrst Payment Default)","text":"<p>O FPD \u00e9 um balizador r\u00e1pido de decis\u00e3o, observamos a primeira parcela em atraso por fatura, ou simplesmente a primeira fatura em atraso, isso quer dizer que independentemente se o valor foi total ou parcialmente pago ele n\u00e3o se configura como FPD. Para avaliarmos os clientes em FPD consideramos apenas quem esteja com atraso a partir de 30 dias ap\u00f3s a data de sua fatura, para assim n\u00e3o contabilizarmos os clientes que por alguma raz\u00e3o atrasaram a conta em alguns dias.</p> <pre><code>                                      \u2753Pagou?\n                                          \u2193\n                                    \u2199           \u2198\n                            \u2705 Sim                 \u274c N\u00e3o\n                            \u2199      \u2198                   \u2193\n                         Total    Parcial              FPD\n</code></pre> Concess\u00e3o Primeira Compra Corte Vencimento D+1 D+30 Creli cliente solicita cr\u00e9dito cliente utiliza o cr\u00e9dito (M0) Data de fechamento da fatura Data de vencimento da fatura Primeiro dia de atraso da fatura 30 dias de atraso da fatura Clientes com mais de 65 dias de atraso"},{"location":"E_glossary/bank_context/fpd/#por-que-a-avaliacao-ocorre-em-m2","title":"Por que a avalia\u00e7\u00e3o ocorre em M2?","text":"<p>A avalia\u00e7\u00e3o \u00e9 feita em M2 pois no m\u00eas da primeira utiliza\u00e7\u00e3o (M0) temos que esperar 30 dias para o fechamento da fatura (que acontecer\u00e1 em M1), ent\u00e3o apenas em M2 teremos os 30 dias de atraso do cliente de M0</p> M0 -&gt; M1 -&gt; M2 M\u00eas da primeira compra fechamento da fatura M\u00eas subsequente a primeira compra (1o vencimento) 30 dias de atraso Segundo m\u00eas ap\u00f3s a primeira compra"},{"location":"E_glossary/bank_context/fpd/#como-calcular-o-fpd","title":"Como calcular o FPD?","text":"<p>Apenas avaliamos os clientes que o m\u00eas da concess\u00e3o coincida com o m\u00eas da primeira utiliza\u00e7\u00e3o (M0), pois como o FPD \u00e9 uma an\u00e1lise de safra, um cliente que teve a primeira utiliza\u00e7\u00e3o apenas 1 ano depois de sua concess\u00e3o, deve ser enquadrar na an\u00e1lise da sua respectiva safra, sendo esse um cliente eleg\u00edvel</p>"},{"location":"E_glossary/bank_context/fpd/#fpd-financeira","title":"FPD financeira","text":"<p>Chamamos de FPD financeiro o percentual do valor da carteira com atraso superior ou igual a 30 dias dentro do universo de clientes eleg\u00edveis</p> <ol> <li>Somar todo o saldo devedor (Saldo devedor total)</li> <li>Somar apenas o saldo devedor a partir de 30 dias de atraso (Saldo devedor em atraso)</li> <li>Dividir o saldo devedor em atraso pelo saldo devedor (FPD)</li> </ol>"},{"location":"E_glossary/bank_context/fpd/#exemplo","title":"Exemplo","text":"cliente saldo devedor atraso XXX.XXX.XXX-XX 1000,00 15 XXX.XXX.XXX-XX 500,00 30 XXX.XXX.XXX-XX 20,00 70 <p>\\(1000 + 50 + 20 + 1070\\)</p> <p>\\(50 + 20 = 70\\)</p> <p>\\(70/1070 = 0.654\\)</p> <p>Dizemos ent\u00e3o que a FPD est\u00e1 em 6,5%</p> <p>Vale observar que existia um saldo devedor de R$1000,00 com 15 dias de atraso e n\u00e3o entrou no c\u00e1lculo da FPD pois n\u00e3o tinha o m\u00ednimo de 30 dias de atraso</p> <pre><code>DT[, sum(saldo_devedor[atraso &gt;= 30])/sum(saldo_devedor)]\n</code></pre>"},{"location":"E_glossary/bank_context/fpd/#fpd-fisico","title":"FPD f\u00edsico","text":"<p>Chamamos de FPD f\u00edsico o percentual de clientes com atraso superior ou igual a 30 dias dentro do universo de clientes eleg\u00edveis</p> <ol> <li>Somar todos os clientes eleg\u00edveis com saldo devedor (Saldo devedor total)</li> <li>Contabilizar apenas os clientes com saldo devedor a partir de 30 dias de atraso, que sejam eleg\u00edveis (Saldo devedor em atraso)</li> <li>Dividir os clientes com saldo devedor em atraso pelo n\u00famero total de clientes (FPD)</li> </ol>"},{"location":"E_glossary/bank_context/fpd/#exemplo_1","title":"Exemplo","text":"cliente saldo devedor atraso fl_elegivel XXX.XXX.XXX-XX 1000,00 15 1 XXX.XXX.XXX-XX 0,00 30 0 XXX.XXX.XXX-XX 20,00 70 1 <p>Clientes com mais de, ou igual a, 30 dias de atraso: 1</p> <p>Clientes eleg\u00edveis: 2</p> <p>1/2 = 0,5</p> <p>Dizemos ent\u00e3o que a FPD est\u00e1 em 50%</p> <p>Vale observar que existia um cliente com 15 dias de atraso e n\u00e3o entrou no c\u00e1lculo da FPD pois n\u00e3o tinha o m\u00ednimo de 30 dias de atraso e o outro cliente que possu\u00eda 40 dias de atraso n\u00e3o entrou na conta pois n\u00e3o era eleg\u00edvel (n\u00e3o utilizou o limite ativado no m\u00eas da concess\u00e3o)</p> <pre><code>DT[, .(fpd = sum(fl_elegivel*(atraso &gt;= 30))/sum(fl_elegivel))]\n</code></pre>"},{"location":"E_glossary/bank_context/ifrs9/","title":"IFRS9","text":"<p>O time de risco de cr\u00e9dito \u00e9 respons\u00e1vel pelo c\u00e1lculo da despesa de provis\u00e3o. A provis\u00e3o \u00e9 uma despesa que impacta na DRE(Demonstra\u00e7\u00e3o de Resultados do Exerc\u00edcio) do banco, diminuindo o lucro(aumento na despesa cont\u00e1vel), mas n\u00e3o gera desembolso.</p>"},{"location":"E_glossary/bank_context/ifrs9/#resolucoes","title":"Resolu\u00e7\u00f5es","text":"<p>2682 de 21/12/1999 4966 de 01/01/2021 356 de 28/11/2023 IFRS9 de 24/07/2014</p>"},{"location":"E_glossary/bank_context/ifrs9/#2682","title":"2682","text":"<p>Resolu\u00e7\u00e3o de 1999 foi aposentada em 01/01/2025 A resolu\u00e7a\u00f5 era muito simples, definindo o crit\u00e9rio m\u00ednimo de provisionamento do banco (AA, A, B, C, D, E, F, G H e HH, sendo HH preju\u00edzo)</p>"},{"location":"E_glossary/bank_context/ifrs9/#4966","title":"4966","text":"<p>(pq no layout do bacen o Motivo da aloca\u00e7\u00e3o do instrumento financeiro no Est\u00e1gio tem vig\u00eancia a partir da data-base janeiro/2026?)</p> <p>\u00c9 o IFRS9 brasileiro Resolu\u00e7\u00e3o publicada em 2021 trazendo o BR GAAP (crit\u00e9rios cont\u00e1beis que regem a contabilidade brasileira).</p> <p>Antes esperavamos o atraso e agora antecipamos o movimento de atraso a partir de m\u00e9tricas que ajudam institui\u00e7\u00f5es finaceiras a quantificar e gerir seu risco de cr\u00e9dito (PD, LGD, EAD e Foward Loop). Esses conceitos, como PD e LGD, vieram depois da crise de 1908 ap\u00f3s verem que os bancos n\u00e3o tinham como se precaver diante de um momento de crise, pois era sempre algo de \"aconteceu, reconheceu\" e n\u00e3o a antecipa\u00e7\u00e3o da ocorr\u00eancia.</p> <p>A resolu\u00e7\u00e3o 4966, assim como a IFRS9 possui \"ess\u00eancia sobre a forma\". Temos liberdade de tomarmos decis\u00e3o da forma que julgarmos mais acertivo para o neg\u00f3cio, dado que consigamos defender nossos pontos. Podemos ter o mesmo LGD para uma carteira de imobili\u00e1rio por exemplo, ou podemos levar em considera\u00e7\u00e3o a rua que o cliente mora, ou sua liquidez, valor do im\u00f3vel, etc. </p> <p>EAD dentro da 4966: Quanto de exposi\u00e7ao a opera\u00e7\u00e3o vai ter se entrar em default. O IFRS9 permite que voc\u00ea estime</p> <p>Principal mudan\u00e7a da 2682 para 4966 \u00e9 considerar a Perda Esperada (agora em cima do limite do cart\u00e3o) para o c\u00e1lculo de provis\u00e3o, ao inv\u00e9s de apenas considerar o saldo devedor</p> <p>Pontos de aten\u00e7\u00e3o:</p> <p>4966 proibe a institui\u00e7\u00e3o de calcular a provis\u00e3o igual da ifrs9, voc\u00ea deve fazer a conta a partir do valor cont\u00e1bil bruto (pr\u00f3ximo ao saldo devedor)</p> <p>Fator de convers\u00e3o de cr\u00e9dito (CCE):</p>"},{"location":"E_glossary/bank_context/ifrs9/#352","title":"352","text":"<p>Resolu\u00e7\u00e3o complementar a 4966. Agora existem os crit\u00e9rios de provis\u00e3o m\u00ednima (...)</p> <p>Estabelece os procedimentos para o c\u00e1lculo da parcela dos ativos ponderados pelo risco (RWA) relativa ao relativa ao c\u00e1lculo do capital requerido para o risco operacional mediante abordagem padronizada</p>"},{"location":"E_glossary/bank_context/ifrs9/#ifrs9","title":"IFRS9","text":"<p>...undo cost effort...</p>"},{"location":"E_glossary/bank_context/ifrs9/#br-gaap-brazilian-generally-accepted-accounting-principles","title":"BR GAAP (Brazilian Generally Accepted Accounting Principles)","text":"<p>BACEN permite que criemos regras, tendo diversas regras a serem seguidas para podermos considerar o cliente como ativo problem\u00e1tico. Apenas o crit\u00e9rio 201 n\u00e3o \u00e9 uma regra objetiva. Indica que voc\u00ea deve fazer uma avalia\u00e7\u00e3o de qual o risco de cr\u00e9dito do cliente hoje comparado com o risco na concess\u00e3o e caso tenha um aumento significativo o cliente deve ser migrado do est\u00e1gio 1 para o 2. Como comprovar a degrada\u00e7\u00e3o</p> <p>O BACEN pede a probabilidade de ativo problem\u00e1tico (cliente entrar em est\u00e1gio 3 por um motivo que n\u00e3o seja o atraso).</p> <p>A receita de cr\u00e9dito vem do Accural de juros, quando considera o cliente como voc\u00ea congela o accural de juros, n\u00e3o podendo mais rolar juros para o cliente, fazendo com que o cliente pare de gerar receita para o banco</p> <p>Muitos crit\u00e9rios n\u00e3o se traduzem para o Brasil. </p> <p>Est\u00e1gio 3 antes de 90 dias de atraso</p> <p>Processo de cura \u00e9 a maior raz\u00e3o para serem migrados em est\u00e1gio 3</p> <p>\u00d3bito \u00e9 reconhecido como preju\u00edzo no m\u00eas em que temos ci\u00eancia do \u00f3bito e deve ser removido da carteira.</p> <p>Uma opera\u00e7\u00e3o pode j\u00e1 nascer no est\u00e1gio 3, por\u00e9m seria estranho conceder cr\u00e9dito a um cliente que tem 100% de PD. A reestrutura\u00e7\u00e3o de alguma outra opera\u00e7\u00e3o que j\u00e1 era considerada PD</p> <p>Um exemplo comum \u00e9: Come\u00e7a no 101, vai para 201 ap\u00f3s 30 dias de atraso e depois vai para o 302. Volta para o 102 e pode ir para 202</p>"},{"location":"E_glossary/bank_context/is_delayed/","title":"Is Delayed","text":"<p>Indicador de rolagem :: Perfil bom que se tornou mau</p> <p>O saldo devedor com mais de X dias de atraso dividido pelo saldo sem atraso que o cliente possu\u00eda no m\u00eas anterior ao atraso</p>"},{"location":"E_glossary/bank_context/is_delayed/#exemplo","title":"Exemplo","text":"<p>Is Delayed 90: Perfil bom no t0 que tornou inadimplente 90 dias depois (rolagem)</p> <p>Is Delayed 7: O saldo devedor adimplente (at\u00e9 5 dias de atraso) no m\u00eas atual constitui o denominador. Observando esse mesmo p\u00fablico, por\u00e9m no m\u00eas seguinte, consideramos todo o saldo com 7 dias ou mais de atraso para o numerador</p>"},{"location":"E_glossary/bank_context/kpi/","title":"KPI (Key Performance Indicator)","text":"<p>KPI, ou Indicador-Chave de Desempenho, s\u00e3o indicadores ou valores que podem ser medidos, comaprados e acompanhados com intuito de apresentar o desempenho dos processos e do trabalho nas estrat\u00e9gias de um neg\u00f3cio. Tem como objetivo dar insumos reais \u00e0s decis\u00f5es de neg\u00f3cio.</p> <p>\"O que n\u00e3o pode ser medido n\u00e3o pode ser gerenciado\" Peter Drucker</p>"},{"location":"E_glossary/bank_context/kpi/#diferenca-entre-metrica-e-kpi","title":"Diferen\u00e7a entre m\u00e9trica e KPI","text":"<p>\u00c9 importante nos atentarmos pois a diferen\u00e7a pode ser sutil em determinadas ocasi\u00f5es, uma vez que as m\u00e9tricas s\u00e3o utilizadas para estruturar os KPIs. Entendemos m\u00e9trica como o dado bruto e KPI \u00e9 a forma que utilizamos esses dados para a avalia\u00e7\u00e3o de performance do neg\u00f3cio.</p>"},{"location":"E_glossary/bank_context/kpi/#exemplo","title":"Exemplo","text":"<ul> <li>CAC: As m\u00e9tricas nesse caso seriam n\u00famero de clientes e valor total gasto com marketing e operacional</li> <li>FPD: As m\u00e9tricas nesse caso seriam quantidade de primeira fatura em atraso, saldo devedor e dias de atraso</li> </ul>"},{"location":"E_glossary/bank_context/ks/","title":"KS (Kolmogorov-Smirnov)","text":"<p>O chamado KS \u00e9 a m</p>"},{"location":"E_glossary/bank_context/npl/","title":"NPL","text":"<p>c</p>"},{"location":"E_glossary/bank_context/pdd/","title":"PDD","text":""},{"location":"E_glossary/bank_context/pdd/#pddmargem","title":"PDD/Margem","text":""},{"location":"E_glossary/bank_context/ratio/","title":"Ratio","text":"<p>O \u00edndice de Basileia \u00e9 um medidor de sa\u00fade financeira dos bancos e institui\u00e7\u00f5es financeiras, e serve para avaliar a capacidade destes em honrar suas obriga\u00e7\u00f5es de pagamento</p> <p>Trata-se de um c\u00e1lculo matem\u00e1tico que foi desenvolvido para medir a exposi\u00e7\u00e3o das insitui\u00e7\u00f5es financeiras ao risco, em compara\u00e7\u00e3o com o valor aportado pelos s\u00f3cios da institui\u00e7\u00e3o. O principal objetivo desse indicador \u00e9 garantir a estabilidade financeira internacional, e reduzindo a probabilidade e a severidade de eventuais crises banc\u00e1rias, e os seus consequentes efeitos para a economia real.</p> 8% 11% 50% \u00cdndice m\u00ednimo exigido pela resolu\u00e7\u00e3o CMN 4.193 \u00cdndice para uma institui\u00e7\u00e3o ser considerada saud\u00e1vel \u00cdndice m\u00e1ximo toletado pelo Banco Central"},{"location":"E_glossary/bank_context/ratio/#quem-regulamenta-o-indice-de-basileia","title":"Quem regulamenta o \u00cdndice de Basileia?","text":"<p>Os requerimentos m\u00ednimos de capital s\u00e3o definidos a partir das recomenda\u00e7\u00f5es do Comit\u00ea de Supervis\u00e3o Banc\u00e1ria de Basileia (Basel COmmittee on Banking Supervision - BCBS), que \u00e9 um f\u00f3rum internacional para discuss\u00e3o e formula\u00e7\u00e3o de recomenda\u00e7\u00f5es para regula\u00e7\u00e3o prudencial e coopera\u00e7\u00e3o para a supervis\u00e3o banc\u00e1ria.</p> <p>O Banco Central do Brasil (BCB) \u00e9 o respons\u00e1vel por assegurar que as recomenda\u00e7\u00f5es sejam aplicadas ao Sistema Financeira Nacional e que a converg\u00eancia da regula\u00e7\u00e3o financeira no pa\u00eds considere as condi\u00e7\u00f5es estruturais da economia brasileira.</p>"},{"location":"E_glossary/bank_context/ratio/#exemplo","title":"Exemplo","text":"<p>Considerando que uma institui\u00e7\u00e3o tenha um \u00edndice de Basileia de 19,6%, isso quer dizer que, de maneira simplificada, para cada R\\(100 emprestado, a institui\u00e7\u00e3o possui R\\)19,60 de patrim\u00f4nio</p> <p>Tendo em vista que a recomenda\u00e7\u00e3o internacional \u00e9 no m\u00ednimo 8% de IB(PR/RWA), podemos observar que quanto menor a Basileia maior o risco para o investidor, uma vez que a institui\u00e7\u00e3o pode estar se arriscando al\u00e9m do recomendado. Mas ao mesmo tempo, um \u00cdndice de Basileia superior a 50% pode representar m\u00e1 gest\u00e3o financeira por parte da institui\u00e7\u00e3o</p>"},{"location":"E_glossary/bank_context/ratio/#como-e-feita-essa-conta","title":"Como \u00e9 feita essa conta?","text":"<p>Patrim\u00f4nio de refer\u00eancia (que \u00e9 o patrim\u00f4nio l\u00edquido cont\u00e1bil da institui\u00e7\u00e3o ajustado conforme defini\u00e7\u00f5es do Banco Central) dividido pelo valor RWA que \u00e9 o valor dos ativos do banco, ponderados pelo risco (diferentes ativos possuem diferentes Fatores Ponderadores de Risco - FPR's)</p>"},{"location":"E_glossary/bank_context/receivables_assignment/","title":"Receivables Assignment","text":"<p>Receivables assignment (also known as assignment of receivables) is a legal transaction in which a person or institution holding the right to receive a payment transfers that right to a third party. The purpose of this transfer is typically to generate profit, either by selling the receivable at a discount or by using it as an investment opportunity.</p> <p>This arrangement allows the original creditor (assignor) to receive funds upfront, while the assignee takes on the right to collect the future payment from the debtor.</p> Portugu\u00eas Ingl\u00eas Definition Cedido Assigned receivable or Receivable The payment or credit right being transferred. Cedente Assignor The original holder of the receivable (usually a creditor or company) who transfers the right. Cession\u00e1rio Assignee The party who receives the receivable and gains the right to collect the payment from the debtor."},{"location":"E_glossary/bank_context/roi/","title":"ROI","text":""},{"location":"E_glossary/bank_context/sla/","title":"SLA","text":""},{"location":"E_glossary/bank_context/tpv/","title":"TPV","text":"<p>IRRBB KPI (Key Performsnce Indicator) LGD (Loss Given Default) LTV (Life Time Value) LTV ( Loan to Value) NIM PGP P&amp;L Prooft PLD RAS ROE ROI Saldo n\u00e3o migrado SCR SLA SQS Swagger TTM UAT</p>"},{"location":"F_models/models/","title":"A","text":"<p>Benchmark with different models in the world</p>"},{"location":"F_models/models/#bank-context","title":"Bank Context","text":""},{"location":"F_models/models/#application-models","title":"Application Models","text":""},{"location":"F_models/models/#collection-models","title":"Collection Models","text":""},{"location":"F_models/application/onboarding_pj/","title":"Onboarding PJ","text":""},{"location":"F_models/behavior/behavior/","title":"Behavior","text":""},{"location":"F_models/collection/creli/","title":"Creli","text":""},{"location":"G_study_notes/convex_optimization/","title":"Convex","text":""},{"location":"G_study_notes/convex_optimization/#teste-e-isso","title":"Teste, \u00e9 isso","text":""},{"location":"G_study_notes/heuristical_optimization/","title":"Heuristical","text":"<p>a</p>"},{"location":"G_study_notes/study_notes/","title":"Study Notes","text":""},{"location":"H_links/links/","title":"Links","text":"<p>MLFlow for Machine Learning Development</p> <p>Capital One Machine Learning</p> <p>Building Nubank YT</p>"},{"location":"I_articles/articles/","title":"Articles","text":""},{"location":"J_references/references/","title":"References","text":"<p>2682</p> <p>IFRS9</p> <p>4966</p> <p>Layout 3040</p> <p>Sklearn Cross Validation</p> <p>Hands-On Large Language Models</p>"}]}