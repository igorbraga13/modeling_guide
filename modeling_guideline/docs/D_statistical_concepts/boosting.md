Boosting √© a fam√≠lia de m√©todos baseados em aprendizado sequencial de modelos fracos (normalmente, √°rvores rasas)

Cada novo modelo tenta corrigir os erros do anterior

No final, os modelos s√£o combinados (por m√©dia ponderada ou soma) para formar um preditor mais forte

‚û°Ô∏è Ideia central: "Vamos aprender com os erros anteriores para melhorar passo a passo."


## AdaBoost

## Gradient Boosting
Gradient Boosting √© uma forma espec√≠fica de Boosting que:

Baseia o processo de corre√ß√£o de erros em gradientes do erro (fun√ß√£o de perda)

Em cada etapa, o novo modelo √© treinado para prever o gradiente negativo do erro (ou seja, a dire√ß√£o de maior melhora na fun√ß√£o de perda)

Isso torna o treinamento mais matematicamente fundamentado e flex√≠vel, pois funciona com qualquer fun√ß√£o de perda diferenci√°vel (ex: MSE, log-loss)


### Gradient Boosting Regressor
O GradientBoostingRegressor implementa o algoritmo cl√°ssico de Gradient Boosting:

Ele treina v√°rias √°rvores de decis√£o sequencialmente, onde cada √°rvore aprende a corrigir os erros cometidos pelas anteriores.

Cada √°rvore √© constru√≠da usando o conjunto completo de amostras e features cont√≠nuas, realizando splits com base em ganhos de informa√ß√£o para valores exatos.

üìâ Esse processo √© preciso, mas computacionalmente caro quando temos muitos dados e valores cont√≠nuos, porque ele precisa testar todos os poss√≠veis pontos de divis√£o.

Usamos quando:
- Seus dados s√£o pequenos/m√©dios
- Voc√™ precisa de precis√£o nos splits
- Quer explica√ß√µes mais detalhadas dos crit√©rios de divis√£o

Em GradientBoostingRegressor e outros modelos como [DecisionTreeRegressor](decision_tree.md), um valor NaN √© um problema, pois o algoritmo precisa comparar valores num√©ricos para fazer os splits, mas NaN n√£o pode ser comparado com operadores como <, >, ==, etc. Desse modo o modelo quebra com erro ou exige que voc√™ impute os valores antes (m√©dia, mediana, etc). Nesse caso podemos utilizar o HistGradientBoostingRegressor

### Histogram Gradient Boosting Regressor
O HistGradientBoostingRegressor √© uma vers√£o otimizada inspirada no LightGBM, introduzida no scikit-learn a partir da vers√£o 0.22.

A grande diferen√ßa:

Ele constr√≥i histogramas para representar intervalos de valores cont√≠nuos ‚Äî ao inv√©s de testar todos os valores poss√≠veis para splits, ele s√≥ testa os limites dos bins.

Isso traz v√°rias vantagens:
‚úÖ Muito mais r√°pido e eficiente com grandes bases.

‚úÖ Suporte nativo a valores faltantes (NaN) ‚Äî eles s√£o tratados como uma categoria separada.

‚úÖ Mem√≥ria muito mais eficiente.

‚ö†Ô∏è Pequena perda de precis√£o nos splits (porque usa aproxima√ß√µes via histogramas).

Usamos quando:

- Voc√™ tem muitos dados ou features cont√≠nuas
- Precisa de performance
- Tem valores ausentes (NaN)
- Est√° acostumado com LightGBM ou CatBoost

Quando o HistGradientBoostingRegressor encontra uma feature com valores faltantes, ele:

1. Cria um grupo separado para os NaNs (eles s√£o um bin separado).
2. Durante o treinamento, ele aprende a onde enviar os NaN no split:
    - Para o lado direito da √°rvore?
    - Para o lado esquerdo?
    - Ele testa ambas as op√ß√µes e escolhe a que resulta em menor erro.

## RusBoost