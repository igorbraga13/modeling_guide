This is the design phase

## üß† Development and Hyperparameter Tuning

The algorithm(s) used, architectures if relevant (e.g., XGBoost, Neural Net).

Train/test splits, metrics used, hyperparameters tuning, validation strategy (CV, OOT, etc).

### Hyperparameter Tuning
A busca de hiperpar√™metro √© importante para...Temos diversas t√©cnicas de busca para otimiza√ß√£o de hiperpar√¢metros

#### Grid Search
O Grid Search testa todas as combina√ß√µes poss√≠veis de hiperpar√¢metros dentro de um grid definido por voc√™.

```python
param_grid = {
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5, 7]
}
```
‚Üí Total de combina√ß√µes: 2 √ó 3 = 6 modelos treinados

‚úÖ Vantagens:

- Explora exaustivamente o espa√ßo definido.
- √ötil quando voc√™ tem poucos par√¢metros e valores espec√≠ficos em mente.

‚ùå Desvantagens:

- Muito lento se o n√∫mero de combina√ß√µes for alto.
- Pode gastar tempo testando combina√ß√µes irrelevantes ou redundantes.

Exemplo de c√≥digo utilizando valida√ß√£o cruzada e RMSE como m√©trica principal

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import make_scorer, mean_squared_error

# Fun√ß√£o de erro RMSE (quanto menor, melhor)
def rmse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred, squared=False)

rmse_scorer = make_scorer(rmse, greater_is_better=False)

# Grade de hiperpar√¢metros
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_iter': [300, 500], #Evite max_iter muito alto no in√≠cio ‚Äî use valores moderados para teste r√°pido.
    'max_leaf_nodes': [15, 31],
    'min_samples_leaf': [10, 20],
    'l2_regularization': [0.0, 1.0]
}

# Grid Search com valida√ß√£o cruzada
grid_search = GridSearchCV(
    estimator=HistGradientBoostingRegressor(),
    param_grid=param_grid,
    scoring=rmse_scorer,
    cv=3,
    verbose=2,
    n_jobs=-1 #usar todos os n√∫cleos do processador.
)

# Executa busca
grid_search.fit(X, y)

# Mostra melhor combina√ß√£o
print("Melhores hiperpar√¢metros encontrados:")
print(grid_search.best_params_)

print(f"Melhor RMSE m√©dio (valida√ß√£o cruzada): {-grid_search.best_score_:.4f}")

# Modelo otimizado
best_model = grid_search.best_estimator_
```

#### Randomized Search

O Randomized Search seleciona aleatoriamente combina√ß√µes de hiperpar√¢metros dentro de um grid definido, por um n√∫mero fixo de tentativas (n_iter).

```python
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_samples_leaf': [10, 20, 30]
}
```
‚Üí Poss√≠veis combina√ß√µes = 3 √ó 3 √ó 3 = 27
‚Üí Mas voc√™ define n_iter=10, ent√£o s√≥ 10 combina√ß√µes aleat√≥rias ser√£o testadas

‚úÖ Vantagens:

- Muito mais eficiente em tempo.
- Pode encontrar bons resultados rapidamente mesmo em grids grandes.
- Permite testar mais par√¢metros simultaneamente com menos custo.

‚ùå Desvantagens:

- Pode n√£o testar a melhor combina√ß√£o poss√≠vel, pois √© aleat√≥rio.
- Resultados variam a cada execu√ß√£o (a menos que defina random_state).

#### Bayesina Optimization
Podemos utilizar o optuna ou hyperopt

#### Convex Algorithms

#### Genetic Algorithms

#### Hyperband

#### Overview

| Situa√ß√£o                                     | Melhor escolha      |
| -------------------------------------------- | ------------------- |
| Poucos hiperpar√¢metros e valores espec√≠ficos | Grid Search       |
| Muitos hiperpar√¢metros ou ranges grandes     | Randomized Search |
| Tempo de execu√ß√£o limitado                   | Randomized Search |
| Busca precisa e exaustiva √© prioridade       | Grid Search       |

## ‚úÖ Validation and Evaluation

Detailed evaluation: metrics results (ROC AUC, F1-score, KS, etc.), important variables.

Desvio padrao da feature para olhar drift feature a feature juntamente de KS, AUC e PR-AUC

https://christophm.github.io/interpretable-ml-book/

### Model Bias and Fairness Analysis

## üí° Interpretation and Explanability